#!/bin/env python
# -*- encoding: utf-8 -*-
import re
import sys
import time
import traceback
from datetime import datetime
from datetime import timedelta 

from pyutil.hiveserver2 import connect
from hive_metastore.ttypes import NoSuchObjectException

Config = {
    "metastore_host": "hive-metastore.m.byted.org",
    "metastore_port": 9083,
}

class HiveThriftContext(object):
    """
    Context manager for hive metastore client.
    """

    def __enter__(self):
        try:
            from thrift.transport import TSocket
            from thrift.transport import TTransport
            from thrift.protocol import TBinaryProtocol
            # Note that this will only work with a CDH release.
            # This uses the thrift bindings generated by the ThriftHiveMetastore service in Beeswax.
            # If using the Apache release of Hive this import will fail.
            from hive_metastore import ThriftHiveMetastore
            transport = TSocket.TSocket(
                            Config["metastore_host"],
                            Config["metastore_port"]
                            )
            transport = TTransport.TBufferedTransport(transport)
            protocol = TBinaryProtocol.TBinaryProtocol(transport)
            transport.open()
            self.transport = transport
            return ThriftHiveMetastore.Client(protocol)
        except ImportError as e:
            raise Exception('Could not import Hive thrift library:' + str(e))

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.transport.close()


class HiveMetastoreClient(object):

    def __init__(self, logger=None):
        self.logger = logger


    def get_table_partitions(self, tbl_name, db_name="default", limit=-1):
        with HiveThriftContext() as client:
            parts = client.get_partitions(
                            db_name=db_name,
                            tbl_name=tbl_name,
                            max_parts=limit
                            )
            return parts


    def get_table_simple_partition_infos(self, tbl_name, db_name="default", limit=-1):
        output = list()
        for part in self.get_table_partitions(tbl_name, db_name, limit):
            tmp_result = {
                "partition_value": part.values,
                "last_ddl_time": part.parameters["transient_lastDdlTime"],
                "total_size": part.parameters["totalSize"],
                "num_rows": part.parameters["numRows"]
                }
            output.append(tmp_result)
        return output


    def get_partition_by_name(self, tbl_name, db_name, part_spec):
        with HiveThriftContext() as client:
            part = client.get_partition_by_name(
                        db_name,
                        tbl_name,
                        part_spec
                        )
            return part

    
    def hive_partition_ready(self, tbl_name, part_spec, db_name="default"):
        if db_name.strip() == "":
            db_name = "default"
        try:
            parts = self.get_partition_by_name(tbl_name, db_name, part_spec)
            gen_time = datetime.strftime(
                    datetime.fromtimestamp(parts.createTime),
                    "%H:%M"
                    )
            return (True, {
                "gen_time": gen_time
            })
        except NoSuchObjectException as e:
            if "table not found" in e.message:
                raise Exception("table not found")
            # ---- deprecated ------
            #if "incomplete partition name" in e.message:
            #    self.logger.info("NoSuchObjectException: partition_name")
            #    partitions = self.get_table_partitions(tbl_name, db_name)
            #    if partitions:
            #        part_values = [part.values[0] for part in partitions]
            #        if part_spec.split('=')[-1] in part_values:
            #            return (True, {})
            return (False, {})
        except Exception as e:
            if self.logger:
                self.logger.info(traceback.format_exc())
            return (False, {})


    def priest_hive_partition_ready(self, tbl_name, db_name, std_time=None, offset=0, partition_name="date", frequency="daily"):
        rt = True
        partition_values = self._priest_get_part_name(frequency, std_time, offset)
        if frequency == "hourly" and  partition_name == "date":
            partition_name = "date_hour"
        for part_value in partition_values:
            part_spec = "{}={}".format(partition_name, part_value)
            if self.logger:
                self.logger.info("Ready: %s %s %s" % (tbl_name, part_spec, db_name))
            try:
                tmp_rt = self.hive_partition_ready(
                    tbl_name=tbl_name,
                    part_spec=part_spec,
                    db_name=db_name
                    )
            except Exception as e:
                self.logger.info(e)
                if "table not found" in e:
                    tmp_rt = (False, e)
            if tmp_rt[0] == True:
                break
        else:
            rt = False
        if self.logger:
            self.logger.info("Ready: %s", rt)
        return rt


    def priest_drop_hive_partition(self, tbl_name, db_name, std_time, offset=0, partition_name="date", frequency="daily", delete_data=True):
        partition_values = self._priest_get_part_name(frequency, std_time, offset)
        if frequency == "hourly" and  partition_name == "date":
            partition_name = "date_hour"
        if db_name.strip() == "":
            db_name = "default"
        with HiveThriftContext() as client:
            for part_value in partition_values:
                part_spec = "{}={}".format(partition_name, part_value)
                if self.logger:
                    self.logger.info(
                        "Drop: %s %s %s" % (tbl_name, part_spec, db_name))
                try:
                    client.drop_partition_by_name(
                            db_name=db_name,
                            tbl_name=tbl_name,
                            part_name=part_spec,
                            deleteData=delete_data
                            )
                    break
                except Exception as e:
                    self.logger.info(traceback.format_exc())
                    continue


    def _priest_get_part_name(self, frequency, std_time, offset):
        if frequency == "daily":
            partition_values = [
                datetime.strftime(std_time - timedelta(days=1-int(offset)), "%Y%m%d"),
                datetime.strftime(std_time - timedelta(days=1-int(offset)), "%Y-%m-%d")
            ]
        elif frequency == "hourly":
            partition_values = [
                datetime.strftime(std_time - timedelta(hours=1-int(offset)), "%Y%m%d_%H"),
                datetime.strftime(std_time - timedelta(hours=1-int(offset)), "%Y%m%d%H")
            ]
        return partition_values


    def priest_drop_tmp_table(self, tbl_name, std_time, offset=0, frequency="daily"):
        partition_values = self._priest_get_part_name(frequency, std_time, offset)
        tbl_name = "{}_tmp_{}".format(tbl_name, partition_values[0])
        self.logger.info("Drop tmp table: %s %s" % ( "test", tbl_name))
        with HiveThriftContext() as client:
            client.drop_table(
                dbname="test",
                name=tbl_name,
                deleteData=True
                )


    def get_schema(self, db_name, table_name):
        output = list()
        with HiveThriftContext() as client:
            schema = client.get_schema(db_name, table_name)
            for field in schema:
                output.append({
                    "name": field.name,
                    "type": field.type,
                    "comment": field.comment
                })
        return output


    def get_all_tables(self, db_name):
        with HiveThriftContext() as client:
            return client.get_all_tables(db_name)


    def get_all_databases(self):
        with HiveThriftContext() as client:
            return client.get_all_databases()
        

    def test(self):
        with HiveThriftContext() as client:
            print dir(client)
            print help(client.get_all_databases)

if __name__ == "__main__":
    client = HiveMetastoreClient(None)
    print client.get_all_databases()

#encoding=utf8
import random
import logging
import time
import uuid
import md5

import impala.dbapi

from pyutil.program.log import config_logging
from pyutil.program.conf import Conf
import pyutil.program.metrics2 as metrics

'''
使用说明：
ImpalaClient增加了连接重试、执行重试、metric、log的功能，支持超时，出错后会抛异常
默认的重试次数较多，重试之间会有间隔

client = ImpalaClient(cluster='default')
client.execute('select * from xxx limit 1')

client.get_table_schema('lizhe_user_activity_daily', 'test')
client.get_table_schema_dict('cube_user_activity_daily')
client.table_exists('cube_fact_user_activity_daily')
client.invalidate_metadata('cube_fact_user_activity_daily')
client.refresh('cube_fact_user_activity_daily')

ImpalaClient.all_clusters_invalidate_metadata('cube_fact_user_activity_daily')
ImpalaClient.all_clusters_refresh('cube_fact_user_activity_daily')
'''

class ImpalaClient(object):

    # reverse mapping of _PrimitiveType_to_TTypeId in impala.rpc
    # FIXME: only for impala 0.8.1, can be removed later 
    TTypeId_to_PrimitiveType = {
        'BOOLEAN_TYPE': 'BOOLEAN',
        'TINYINT_TYPE': 'TINYINT',
        'SMALLINT_TYPE': 'SMALLINT',
        'INT_TYPE': 'INT',
        'BIGINT_TYPE': 'BIGINT',
        'TIMESTAMP_TYPE': 'TIMESTAMP',
        'FLOAT_TYPE': 'FLOAT',
        'DOUBLE_TYPE': 'DOUBLE',
        'STRING_TYPE': 'STRING',
    }
    METRIC_PREFIX = 'inf.impalaclient'

    conf = Conf('/opt/tiger/ss_conf/ss/impala.conf')
    logger = logging.getLogger('impala_client')
    _inited = False

    def __init__(self, cluster='default', servers='',
        timeout=600, timeout_factor=1.2, connect_timeout=3,
        conn_max_tries=30, conn_try_interval=1,
        execute_max_tries=2, execute_try_interval=1,
        log_filename=None):
        '''
        timeout_factor: 每次重试后超时的增长因子，目前只影响执行阶段
        '''

        self.cluster = cluster # also used for metric
        if servers:
            self.servers = servers.split(',')
            if not self.servers:
                raise ValueError('no server found in servers')
        else:
            self.servers = self.conf.get_values('%s_servers' % self.cluster)
            if not self.servers or not self.servers[0]: # empty string
                raise ValueError('no servers found for cluster %s' % self.cluster)

        self.timeout = timeout
        self.timeout_factor = timeout_factor;
        self.connect_timeout = connect_timeout;

        self.conn_max_tries = conn_max_tries
        self.conn_try_interval = conn_try_interval
        self.execute_max_tries = execute_max_tries
        self.execute_try_interval = execute_try_interval

        if not ImpalaClient._inited:
            ImpalaClient._init_cls(log_filename)
            ImpalaClient._inited = True

        try:
            impala.dbapi.set_internal_tries(1)
        except:
            pass

    @classmethod
    def _get_cur_host(self):
        try:
            import socket
            return socket.gethostbyname(socket.gethostname())
        except:
            return ''

    @classmethod
    def _init_cls(cls, log_filename):
        # init metrics
        metrics.define_counter('count', prefix=cls.METRIC_PREFIX)
        metrics.define_counter('fail', prefix=cls.METRIC_PREFIX)
        metrics.define_timer('latency', prefix=cls.METRIC_PREFIX)

        clusters = cls.conf.get_values('all_clusters')
        metrics.define_tagkv('cluster', clusters)
        for c in clusters:
            servers = cls.conf.get_values('%s_servers' % c)
            servers = [x.replace(':', '_') for x in servers]
            metrics.define_tagkv('server', servers)

        # init logger
        config_logging(logger=cls.logger, filename=log_filename,
            format='%(asctime)s %(thread)d %(levelname)-7s [' + cls._get_cur_host() + '] %(message)s',
            level=logging.DEBUG, category='impala_client')

    def _get_conn(self, timeout):
        # we should use short connection with impala
        ok = False
        ts1 = time.time()
        for tried in range(self.conn_max_tries):
            if tried > 0 and self.conn_try_interval > 0:
                time.sleep(self.conn_try_interval)
            ts2 = time.time()
            try:
                addr = random.choice(self.servers)
                host, port = addr.split(':')
                port = int(port)
                conn = impala.dbapi.connect(host=host, port=port,
                        timeout=timeout,
                        connect_timeout=self.connect_timeout)
                ok = True
                return addr, conn
            except:
                if tried + 1 == self.conn_max_tries:
                    raise
            finally:
                ts3 = time.time()
                self.logger.log(logging.DEBUG if ok else logging.WARN,
                    ('connect %s. tried %d/%d cluster %s server %s '
                        + 'cost_cur %.6f cost_total %.6f connect_timeout %.3f '
                        + 'try_interval %.3f'),
                    'ok' if ok else 'failed', tried + 1, self.conn_max_tries,
                    self.cluster, addr, ts3 - ts2, ts3 - ts1,
                    self.connect_timeout, self.conn_try_interval)

        raise Exception('should not reach here')

    def _release_conn(self, conn, cursor=None):
        try:
            if cursor:
                cursor.close()
            if conn:
                conn.close()
        except:
            self.logger.exception('release conn failed')

    def _execute(self, cb, query_text):
        addr = ''
        conn = None
        cursor = None
        tags = {'cluster': self.cluster}
        ok = False

        query_id = uuid.uuid1().hex
        query_sign = md5.new(query_text).hexdigest()

        ts1 = time.time()
        self.logger.debug('execute start. cluster %s query_id %s query_sign %s query: %s',
            self.cluster, query_id, query_sign, query_text)
        for tried in range(self.execute_max_tries):
            if tried > 0 and self.execute_try_interval > 0:
                time.sleep(self.execute_try_interval)
            timeout = self.timeout * pow(self.timeout_factor, tried)
            ts2 = time.time()
            try:
                addr, conn = self._get_conn(timeout)
                tags['server'] = addr.replace(':', '_')
                cursor = conn.cursor()

                result = cb(conn, cursor, timeout)

                ok = True
                return result
            except:
                metrics.emit_counter('fail', 1, self.METRIC_PREFIX, tags)
                self.logger.exception('execute failed')
                if tried + 1 == self.execute_max_tries:
                    raise
            finally:
                self._release_conn(conn, cursor)

                ts3 = time.time()
                self.logger.log(logging.INFO if ok else logging.WARNING,
                    ('execute %s. tried %d/%d cluster %s server %s '
                    + 'cost_cur %.3f cost_total %.3f timeout %.3f timeout_actual %.3f '
                    + 'try_interval %.3f query_id %s query_sign %s query: %s'),
                    'ok' if ok else 'failed', tried + 1, self.execute_max_tries,
                    self.cluster, addr, ts3 - ts2, ts3 - ts1, self.timeout,
                    timeout, self.execute_try_interval, query_id, query_sign, query_text)

                metrics.emit_counter('count', 1, self.METRIC_PREFIX, tags)
                metrics.emit_timer('latency', int((ts3 - ts2) * 1e6), self.METRIC_PREFIX, tags)

        raise Exception('should not reach here')

    def _execute_sql(self, sql, include_resp=True, include_desc=False):
        def cb(conn, cursor, timeout):
            cursor.execute(sql, timeout=timeout)
            if include_resp:
                if include_desc:
                    return cursor.description, cursor.fetchall()
                else:
                    return cursor.fetchall()
        return self._execute(cb, sql)

    def _execute_on_table(self, sql, table_name, database):
        if database and table_name:
            table_name = '%s.%s' % (database, table_name)
        if table_name:
            sql += ' ' + table_name
        return self._execute_sql(sql, include_resp=False)

    def invalidate_metadata(self, table_name=None, database=None):
        return self._execute_on_table('INVALIDATE METADATA', table_name, database)

    def refresh(self, table_name=None, database=None):
        return self._execute_on_table('REFRESH', table_name, database)

    @classmethod
    def all_clusters_invalidate_metadata(cls, table_name=None, database=None):
        clusters = cls.conf.get_values('all_clusters')
        for cluster in clusters:
            client = ImpalaClient(cluster=cluster)
            client.invalidate_metadata(table_name, database)

    @classmethod
    def all_clusters_refresh(cls, table_name=None, database=None):
        clusters = cls.conf.get_values('all_clusters')
        for cluster in clusters:
            client = ImpalaClient(cluster=cluster)
            client.refresh(table_name, database)

    def execute(self, sql, include_resp=True, include_desc=False):
        return self._execute_sql(sql, include_resp=include_resp, include_desc=include_desc)

    def get_table_schema(self, table_name, database=None):
        def cb(conn, cursor, timeout):
            return cursor.get_table_schema(table_name, database)
        result = self._execute(cb, 'get_table_schema')
        return [(f, self.TTypeId_to_PrimitiveType.get(t, t)) for f, t in result]

    def get_table_schema_dict(self, table_name, database=None):
        return {f: t for f, t in self.get_table_schema(table_name, database)}

    def table_exists(self, table_name, database=None):
        def cb(conn, cursor, timeout):
            return cursor.table_exists(table_name, database)
        return self._execute(cb, 'table_exists')

if __name__ == '__main__':
    #client = ImpalaClient(servers='10.100.6.68:1122', timeout=5, connect_timeout=0.3, log_filename='./tmp.log')
    #print client.get_table_schema('lizhe_user_activity_daily', 'test')

    #time.sleep(10)
    #client = ImpalaClient(servers='in161-004:1215,10.4.161.4:9215')
    #client = ImpalaClient(servers='10.4.161.4:9215')
    #client = ImpalaClient(cluster='default')
    #print client.execute('show create table test.lizhe_user_activity_daily')
    #print client.get_table_schema('lizhe_user_activity_daily', 'test')
    #print client.get_table_schema_dict('cube_user_activity_daily')
    #print client.table_exists('cube_user_activity_daily')
    #print client.table_exists('cube_fact_user_activity_daily')
    #print client.invalidate_metadata('cube_fact_user_activity_daily')
    #print client.refresh('cube_fact_user_activity_daily')

    #client = ImpalaClient(cluster='nearline', timeout=3)
    client = ImpalaClient(cluster='nearline', timeout=30, connect_timeout=0.3, log_filename='./tmp.log')
    #print client.execute("select count(distinct t1.ut,t1.uid) from server_refresh_daily as t1 join user_external_property_daily as t2 on (t1.`date`='20150506' and t2.`date`='20150506' and t1.ut=t2.ut and t1.uid=t2.uid and (t2.impr_types like '%feed%' or t2.impr_types like '%channel%') and t2.device_id <>0 and t2.ut in (12,14) and (t1.impr_type like '%feed%' or t1.impr_type like '%channel%') and device_id % 2000 >= 120 and device_id % 2000 < 140)")
    print client.execute("SELECT COUNT(distinct `user_uid`, `user_uid_type`, `group_id`), `platform_type` FROM impression_stats_daily WHERE `impr_time` > 0 AND `user_access` <> 10 AND `impr_source`='f' AND `user_uid_type` <> 13 AND `impr_from` = '' AND `date` <= '20150620' AND `traffic_type` = 'app' AND `date` >= '20150620' GROUP BY `platform_type`")

    #client = ImpalaClient(cluster='default2')

    #ImpalaClient.all_clusters_invalidate_metadata('cube_fact_user_activity_daily')


#coding=utf-8
import logging, six, time, redis
from collections import defaultdict
from kafka import SimpleConsumer
from kafka.common import FetchRequest, ConsumerFetchSizeTooSmall, ConsumerNoMoreData, OffsetOutOfRangeError
from pyutil.program import functool
from util import extend_message

class MessageAcker(object):
    """
    redis_client: a connected redis client to store offsets of pending messages. If None, do nothing
    """

    REDIS_PENDING_KEY = '{prefix}/{topic}/{group}/{partition}'

    def __init__(self, redis_client, pending_key_prefix='kafka_pending'):
        self.redis_client = redis_client
        self.pending_key_prefix = pending_key_prefix

    @functool.retries(8)
    def ack(self, topic, group, partition, *offsets):
        """
        Acknowledge pending messages
        """
        if not self.redis_client: return
        self.redis_client.zrem(self._get_pending_key(topic, group, partition), *offsets)

    def ack_by_meta(self, meta):
        """
        :param MessageMeta meta:
        """
        return self.ack(meta.topic, meta.group, meta.partition, meta.offset)

    @functool.retries(4)
    def pend(self, topic, group, partition, *offsets):
        """
        Acknowledge pending messages
        """
        if not self.redis_client: return
        ts = time.time()
        offset_score = dict((str(offset), ts) for offset in offsets)
        self.redis_client.zadd(self._get_pending_key(topic, group, partition), **offset_score)

    def clear_pending(self, topic, group, partition):
        if not self.redis_client: return
        self.redis_client.delete(self._get_pending_key(topic, group, partition))

    def get_pending_offsets(self, topic, group, partitions, count):
        """
        Return [(partition, offset), ...]
        """
        if not self.redis_client: return []
        if not count or not partitions: return []
        if count > 0:
            pcount = max(1, int(count / len(partitions)))
        else:
            pcount = count

        offsets = []
        for p in partitions:
            offsets_ = self.redis_client.zrange(self._get_pending_key(topic, group, p), 0,
                    -1 if pcount < 0 else pcount - 1, withscores=True)
            offsets += [(p, int(offset), int(ts)) for offset, ts in offsets_]
        return offsets

    def get_pending_num(self, topic, group, partitions):
        """
        Return pending_num
        """
        if not self.redis_client: return 0
        pending_num = 0
        for p in partitions:
            num = self.redis_client.zcard(self._get_pending_key(topic, group, p)) or 0
            pending_num += num
        return pending_num

    def _get_pending_key(self, topic, group, partition):
        return self.REDIS_PENDING_KEY.format(prefix=self.pending_key_prefix, topic=topic,
            group=group, partition=partition)

class AckConsumer(SimpleConsumer):
    """
    A consumer implementation that consumes all/specified partitions
    for a topic and supports message acknowledgement.

    acker: see MessageAcker
    client: a connected KafkaClient
    group: a name for this consumer, used for offset storage and must be unique
    topic: the topic to consume
    partitions: An optional list of partitions to consume the data from

    fetch_size_bytes:    number of bytes to request in a FetchRequest
    buffer_size:         Initial number of bytes to tell kafka we
                         have available. This will double as needed.
    max_buffer_size:     Max number of bytes to tell kafka we have
                         available. None means no limit.
    transform_message: a callback to transform message result(topic, group, partition, offset, message). if True, use extend_message
    """

    def __init__(self, acker, client, group, topic, partitions=None, transform_message=None, message_format='json', **kwargs):
        kwargs.pop('auto_commit', None)
        SimpleConsumer.__init__(self, client, group, topic,
                partitions=partitions, auto_commit=False,
                **kwargs
                )
        self.partition_info = True

        self.transform_message = extend_message if transform_message is True else transform_message
        self.message_format = message_format

        self.acker = acker

    def __repr__(self):
        return '<AckConsumer group=%s, topic=%s, partitions=%s>' % \
                (self.group, self.topic, str(self.offsets.keys()))


    def get_messages(self, count=1, block=True, timeout=0.1):
        """
        Fetch the specified number of messages

        count: Indicates the maximum number of messages to be fetched
        block: If True, the API will block till some messages are fetched.
        timeout: If block is True, the function will block for the specified
                 time (in seconds) until count messages is fetched. If None,
                 it will block forever.
        """

        messages = SimpleConsumer.get_messages(self, count=count, block=block, timeout=timeout)
        p2offsets = defaultdict(list)
        for partition, message in messages:
            p2offsets[partition].append(message.offset)
        for p, offsets in p2offsets.items():
            self.acker.pend(self.topic, self.group, p, *offsets)
        self.commit()
        return self._transform_messages(messages)

    def ack(self, partition, *offsets):
        """
        Acknowledge pending messages
        """

        self.acker.ack(self.topic, self.group, partition, *offsets)

    def clear_pending(self):
        """ Clear all pending messages.
        """
        for p in self.offsets.keys():
            self.acker.clear_pending(self.topic, self.group, p)

    def get_pending_num(self):
        """
        Return pending_num
        """

        return self.acker.get_pending_num(self.topic, self.group, self.offsets.keys())

    def get_pending_offsets(self, count=-1):
        """
        Return [(partition, offsets), ...]
        """
        return self.acker.get_pending_offsets(self.topic, self.group, self.offsets.keys(), count)

    def get_pending_messages(self, count=-1, timeout=0, buffer_size=10240, max_wait_time=10, min_bytes=4096):
        """
        Fetch the specified number of messages

        count: Indicates the maximum number of messages to be fetched, -1 for unlimited
        timeout: fetch the message which pending time bigger than timeout (seconds)
        buffer_size : buffer_size of kafka fetch request
        max_wait_time : max_wait_time of kafka send_fetch_request
        min_bytes : min_bytes of kafka send_fetch_request
        """

        messages = []
        now_ts = time.time()
        pending_offsets = self.get_pending_offsets(count)
        """
        cache mechanism
        1. peek more messages from kafka
        2. put all offset into cache_messages dict;
        3. if offset is in the cache_messages, get message from cache_messages directly, else get messages from kafka
            goto 1.
        """
        cache_messsages = {}
        p2offsets = defaultdict(list)
        missing_p2offsets = defaultdict(list)

        for p, offset, ts in pending_offsets: 
            p_offset = "%d_%d" % (p, offset)
            if (now_ts - ts) >= timeout:
                if p_offset in cache_messsages:
                    messages.append(cache_messsages[p_offset])
                    p2offsets[p].append(offset)
                else:
                    try:
                        messages_ = self.__peek_messages(p, offset, buffer_size, max_wait_time, min_bytes)
                    except OffsetOutOfRangeError:
                        messages_ = []
                    for message in messages_:
                        key = "%d_%d" % (p, message[1].offset)
                        cache_messsages[key] = message
                    if p_offset in cache_messsages: # 可能已归档
                        messages.append(cache_messsages[p_offset])
                        p2offsets[p].append(offset)
                    else:
                        missing_p2offsets[p].append(offset)
        for p, offsets in missing_p2offsets.items():
            logging.warn('partition=%s discard %s no-message offsets(%s)',
                    p, len(offsets), ','.join(map(str, offsets[:10])))
            self.ack(p, *offsets)
        for p, offsets in p2offsets.items():
            self.acker.pend(self.topic, self.group, p, *offsets)

        return self._transform_messages(messages)

    def peek_messages(self, partition, offset, buffer_size=10240, max_wait_time=10, min_bytes=4096):
        try:
            results = self.__peek_messages(partition, offset, buffer_size, max_wait_time, min_bytes)
        except OffsetOutOfRangeError as e:
            logging.exception(e)
            results = []
        return self._transform_messages(results)

    def __peek_messages(self, partition, offset, buffer_size, max_wait_time, min_bytes):
        results = []
        fetch_offsets = {partition: offset}
        buffer_size = buffer_size or self.buffer_size
        # Create fetch request payloads for all the partitions
        partitions = dict((p, buffer_size) for p in [partition])
        while partitions:
            requests = []
            for partition, buffer_size in six.iteritems(partitions):
                requests.append(FetchRequest(self.topic, partition,
                                             fetch_offsets[partition],
                                             buffer_size))
            # Send request
            responses = self.client.send_fetch_request(
                requests,
                max_wait_time=max_wait_time,
                min_bytes=min_bytes)

            retry_partitions = {}
            for resp in responses:
                partition = resp.partition
                buffer_size = partitions[partition]
                try:
                    for message in resp.messages:
                        if message.offset >= offset:
                            results.append((partition, message))
                except ConsumerFetchSizeTooSmall:
                    if (self.max_buffer_size is not None and
                            buffer_size == self.max_buffer_size):
                        logging.error("Max fetch size %d too small",
                                  self.max_buffer_size)
                        raise
                    if self.max_buffer_size is None:
                        buffer_size *= 2
                    else:
                        buffer_size = min(buffer_size * 2,
                                          self.max_buffer_size)
                    logging.warn("Fetch size too small, increase to %d (2x) "
                             "and retry", buffer_size)
                    retry_partitions[partition] = buffer_size
                except ConsumerNoMoreData as e:
                    logging.debug("Iteration was ended by %r", e)
                except StopIteration:
                    # Stop iterating through this partition
                    logging.debug("Done iterating over partition %s" % partition)
            partitions = retry_partitions

        return results

    def peek_message(self, partition, offset, max_message_size=None):
        messages = self.peek_messages(partition, offset, buffer_size=max_message_size)
        if messages and messages[0].meta.offset == offset:
            return messages[0]
        else:
            return None

    def get_message(self, block=True, timeout=0.1, get_partition_info=None):
        raise NotImplementedError('get_message function is not implemented')

    def _transform_messages(self, messages):
        if self.transform_message:
            messages = [self.transform_message(self.topic, self.group, partition, message.offset, message.message, self.message_format)
                    for partition, message in messages]
        return messages

    def __iter__(self):
        raise NotImplementedError('not iterable')

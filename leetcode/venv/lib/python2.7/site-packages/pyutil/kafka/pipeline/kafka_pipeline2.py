# coding=utf-8
import time
import logging
import multiprocessing
import socket
from pyutil.kafka import AckConsumer
from pyutil.kafka.consumer import MessageAcker
from pyutil.program import timing
from pyutil.program.fmtutil import fmt_exception
from pyutil.program.functool import keep_run
from pyutil.program.hash_utils import hash_uint64
from pyutil.program.keyed_thread_pool import KeyedThreadPool
from pyutil.program.python import split_chunks
from pyutil.program.tracing import start_tracer, start_span
from pyutil.kafka.pipeline.kafka_utils import get_kafka_partitions, get_redis_client, get_kafka_client
from pyutil.kafka.pipeline.kafka_pipeline import (
        ReadKafkaTaskProducer, BatchHandler, Handler, _get_message_log_id, DEFAULT_KAFKA_BATCH_SIZE,
        )

logger = logging

class KafkaPipeline2(object):
    '''
    每个partition一个进程(worker)，避免producer和threadpool间使用multiprocess queue以提升性能
    '''
    def __init__(self, name, conf, handle,
            kafka_topics, kafka_group,
            kafka_cluster, acker_cluster,
            kafka_batch_size=DEFAULT_KAFKA_BATCH_SIZE,
            batch_process=False,
            batch_size=0,
            hosts=None,
            host_partitions=None,
            worker_reload=None, worker_reload_interval=1,
            worker_thread_num=None,
            get_message_log_id=None,
            dry_run=False,
            message_format='json',
            pending_full_discard=False,
            ):
        '''
        :param str name: pipeline name
        :param callable handle: 若batch_process为True, 则handle(messages), 否则handle(message)
        :param int worker_thread_num: thread num for each worker.
        :param list kafka_topics:
            'topic1'
            or ['topic1', 'topic2']
        :param list hosts: pipeline部署的机器列表, 用于推导host_partitions
        :param list host_partitions: pipeline在当前host消费的partitions
        :param str kafka_cluster: kafka cluster name
        :param int kafka_batch_size: 每次从kafka中获取的消息数
        :param str acker_cluster: redis acker cluster name, if Noen, acker does nothing
        :param bool batch_process: 是否批处理任务
        :param int batch_size: 批处理的message数, 0表示不限. 同一batch的messages的key必须相同
        :param callable worker_reload: worker周期性执行的函数
        :param int worker_reload_interval: 加载间隔(s)
        :param callable get_message_log_id: get_message_log_id(message)

        kafka_partitions和hosts不能同时提供
        '''
        timer = timing.Timer()
        if isinstance(kafka_topics, basestring):
            kafka_topics = [kafka_topics]
        if host_partitions and hosts:
            raise ValueError('host_partitions and hosts can not be provided at the same time')
        host = socket.gethostname()
        if hosts and host not in hosts:
            raise ValueError('%s not in given hosts: %s' % (host, ','.join(hosts)))
        get_message_log_id = get_message_log_id or _get_message_log_id
        self.message_acker = MessageAcker(get_redis_client(conf, name=acker_cluster, dry_run=dry_run) if acker_cluster else None)
        handle_classs = BatchHandler if batch_process else Handler
        handler = handle_classs(handle, self.message_acker, name)
        timer.timing('misc')
        kafka_client = get_kafka_client(conf, kafka_cluster)
        self.topic_partitions = self._get_topic_partitions(kafka_client, kafka_topics)
        timer.timing('kafka_client')
        self.kafka_partition_processess = []
        if hosts:
            host_partitions = get_kafka_partitions(hosts, len(self.topic_partitions))
        host_partitions = host_partitions or self.topic_partitions

        list_of_partitions = split_chunks(1, host_partitions)
        n = len(list_of_partitions)
        for i, partitions in enumerate(list_of_partitions):
            pool_name = '%s_p%s' % (name, partitions[0])
            thread_pool = KeyedThreadPool(pool_name, handler.process,
                    thread_num=worker_thread_num,
                    post_fork_reload=worker_reload,
                    reload_interval=worker_reload_interval,
                    metric_name=name,
                    key_hash=hash_uint64, # 使用同kafka producer不同的hash函数, 避免分发不均匀
                    pending_full_discard=handler.discard if pending_full_discard else None,
                    )
            timer.timing('worker_pool')
            kp_process = KafkaPartitionProcess(
                    conf=conf,
                    thread_pool=thread_pool,
                    message_acker=self.message_acker,
                    kafka_topics=kafka_topics,
                    kafka_group=kafka_group,
                    kafka_partitions=partitions,
                    kafka_client=kafka_client,
                    message_format=message_format,
                    kafka_batch_size=kafka_batch_size or DEFAULT_KAFKA_BATCH_SIZE,
                    batch_process=batch_process,
                    batch_size=batch_size,
                )
            kp_process.daemon = True
            self.kafka_partition_processess.append(kp_process)
            timer.timing('kafka_consumer')
        logger.info('pipeline %s constructed, partitions=%s-%s/%s partition_threads=%s kafka_group=%s, dur(%s)', name,
                host_partitions[0], host_partitions[-1], len(self.topic_partitions),
                worker_thread_num,
                kafka_group,
                timer.pformat())

    def _get_topic_partitions(self, kafka_client, kafka_topics):
        first_topic = kafka_topics[0]
        kafka_client.load_metadata_for_topics(first_topic)
        return kafka_client.topic_partitions[first_topic]

    def start(self):
        start_tracer()
        [x.start() for x in self.kafka_partition_processess]


class KafkaPartitionProcess(multiprocessing.Process):
    def __init__(self, conf, thread_pool, message_acker, kafka_topics, kafka_group,
                 kafka_partitions, get_message_log_id=_get_message_log_id,
                 cluster_name=None, kafka_client=None, max_throughput=0, message_format='json',
                 kafka_batch_size=DEFAULT_KAFKA_BATCH_SIZE, batch_process=False, batch_size=0,
                 *args, **kwargs):
        '''
        :param kafka_client:
        :param str cluster_name: kafka cluster name. kafka_client和cluster_name必须给一个，优先使用kafka_client
        :param callable put_task: put_task(task_key, task, task_log_id)
        :param list kafka_topics:
        :param int kafka_batch_size: see KafkaPipeline
        :param int batch_size: see KafkaPipeline
        '''
        multiprocessing.Process.__init__(self, *args, **kwargs)
        if not cluster_name and not kafka_client:
            raise ValueError('cluster_name or kafka_client must be provided')
        self.conf = conf
        self.thread_pool = thread_pool
        self.put_task = thread_pool.put_task
        self._get_message_log_id = get_message_log_id
        self.kafka_topics = kafka_topics
        self.kafka_group = kafka_group
        self.kafka_partitions = kafka_partitions
        self.max_throughput = max_throughput
        self.message_format = message_format
        self.batch_process = batch_process
        self.batch_size = batch_size
        self.kafka_batch_size = kafka_batch_size
        self._message_counter = 0
        if not kafka_client:
            kafka_client = get_kafka_client(conf, cluster_name)
        self._kafka_consumers = []

        for ktopic in kafka_topics:
            partitions = self.kafka_partitions[ktopic] if type(self.kafka_partitions) == dict else self.kafka_partitions
            kafka_consumer = AckConsumer(message_acker, kafka_client,
                                         self.kafka_group, ktopic,
                                         partitions=partitions, transform_message=True,
                                         message_format=message_format,
            )
            self._kafka_consumers.append(kafka_consumer)


    @keep_run(30)
    def run(self):
        self.thread_pool.start()
        logging.info('start task producer, topics=%s, partitions=%s',
                self.kafka_topics, self.kafka_partitions)
        last_pend_ts = 0
        pending_timeout = 0
        while True:
            try:
                with start_span('producer_root') as span:
                    last_pend_ts, pending_timeout = self._run_once(span, last_pend_ts, pending_timeout)
            except Exception as e:
                logging.exception('unexpected: %s', fmt_exception(e))
                with start_span('producer_exception_sleep'):
                    time.sleep(30)

    _run_once = ReadKafkaTaskProducer.__dict__['_run_once']
    _put_messages = ReadKafkaTaskProducer.__dict__['_put_messages']

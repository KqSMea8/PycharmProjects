#!/usr/bin/env python
#coding: utf-8

import os, logging, random
import kafka_proxy
import time

from kafka.common import (
    ErrorMapping, FetchRequest,
    OffsetRequest, OffsetFetchRequest, OffsetCommitRequest,
    ConsumerFetchSizeTooSmall, ConsumerNoMoreData
)
from pyutil.program.conf import Conf

class KafkaSpy(kafka_proxy.KafkaProxy):
    def __init__(self, topic, buffer_size=None, partitions=None, consumer_group=None, conf=None, debug=False,
                 cluster_name=None, partition_cache_period=120):
        if not conf:
            conf = Conf("/opt/tiger/ss_conf/ss/kafka.conf")
        if cluster_name is None:
            from kafka_router import get_cluster
            cluster_name = get_cluster(topic)
        kafka_proxy.KafkaProxy.__init__(self, topic, buffer_size, partitions, consumer_group, conf=conf, debug=debug,cluster_name=cluster_name)
        self.topic = topic
        self.partitions = None
        self.partition_cache_period = partition_cache_period
        self.next_update_partition_time = 0
        self.client = self.get_kafka_client()
        self.client.load_metadata_for_topics(topic)

    def get_metadata(self):
        return self.client.spy_metadata_for_topics(self.topic)

    def get_offsets(self, group, ignore_failure=False):
        if not self.partitions:
            self.partitions = self.client.topic_partitions[self.topic]
        def callback(resp):
            if resp.error == ErrorMapping.NO_ERROR:
                return resp.offset
            else:
                return 0

        offsets = {}
        for partition in self.partitions:
            req = OffsetFetchRequest(self.topic, partition)
            try:
                (offset,) = self.client.send_offset_fetch_request(group, [req], callback=callback, fail_on_error=False)
            except Exception, e:
                if ignore_failure:
                    logging.error(str(e))
                    offset = -1
                else:
                    raise e
            offsets[partition] = offset
        return offsets

    def get_padding(self):
        if not self.partitions or time.time() > self.next_update_partition_time:
            self.client.load_metadata_for_topics(self.topic)
            if self.topic not in self.client.topic_partitions:
                logging.warning("Topic %s not find.", self.topic)
                return None
            self.next_update_partition_time = time.time() + self.partition_cache_period
            self.partitions = self.client.topic_partitions[self.topic]
        reqs = []
        for partition in self.partitions:
            reqs.append(OffsetRequest(self.topic, partition, -1, 1))
        resps = self.client.send_offset_request(reqs)
        return resps

def main():
    topic = "online_tera_feature"
    spyer = KafkaSpy(topic,cluster_name="kafka_huge")
    brokers, topics = spyer.get_metadata()
    print "brokers\n", brokers
    print "topics\n", topics
    offsets = spyer.get_offsets("import_action_kafka_to_hbase")
    print "offsets\n", offsets
    paddings = spyer.get_padding()
    print "pedding\n", paddings

if __name__ == "__main__":
    main()


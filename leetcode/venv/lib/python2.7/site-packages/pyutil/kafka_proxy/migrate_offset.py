#!/usr/bin/python
import argparse, sys
from pyutil.kafka_proxy.kafka_proxy import KafkaProxy
from kafka.common import OffsetRequest, OffsetFetchRequest, OffsetCommitRequest

def parse_args():
    parser = argparse.ArgumentParser(description='Migrate kafka offset between clusters')

    group = parser.add_argument_group('topic')
    group.add_argument('-c', '--source-cluster', type=str, required=True, help='source cluster')
    group.add_argument('-t', '--topic', type=str, required=True, help='topic name')
    group.add_argument('-C', '--target-cluster', type=str, required=True, help='target cluster')
    group.add_argument('-g', '--group', type=str, required=True, help='consumer group id')
    group.add_argument('-G', '--target-group', type=str, required=False, help='target consumer group id')
    group.add_argument('-p', '--partitions', type=str, required=False, default='', help='partitions')
    group.add_argument('-e', '--execute', action='store_true', default=False, help='execute it')

    if len(sys.argv) <= 1:
        parser.print_help()
        sys.exit(-1)
    args = parser.parse_args()
    return args

def read_consumer_offset(cluster_name, topic, group, enforce_partitions):
    proxy = KafkaProxy(topic=topic, cluster_name=cluster_name)
    client = proxy.get_kafka_client()
    try:
        client.load_metadata_for_topics(topic)
        partitions = client.topic_partitions[topic]
        if enforce_partitions is not None:
            partitions = enforce_partitions
        resps = client.send_offset_request([OffsetRequest(topic, p, -1, 1) for p in partitions])
        topic_offsets = {resp.partition: resp.offsets[0] for resp in resps}

        resps = client.send_offset_fetch_request(
            group,
            [OffsetFetchRequest(topic, p) for p in partitions],
            fail_on_error=True
        )
        group_offsets = {resp.partition: max(0, resp.offset) for resp in resps}
        return topic_offsets, group_offsets
    finally:
        client.close()

def write_consumer_offset_delta(cluster_name, topic, group, delta, execute, enforce_partitions):
    proxy = KafkaProxy(topic=topic, cluster_name=cluster_name)
    client = proxy.get_kafka_client()
    try:
        client.load_metadata_for_topics(topic)
        partitions = client.topic_partitions[topic]
        if enforce_partitions is not None:
            partitions = enforce_partitions
        resps = client.send_offset_request([OffsetRequest(topic, p, -1, 1) for p in partitions])
        topic_offsets = {resp.partition: resp.offsets[0] for resp in resps}
        if len(partitions) != len(delta):
            raise Exception('Number of partitions mismatch')
        new_offsets = {p: topic_offsets[p] - delta[p] for p in partitions}
        print 'new offsets: %s' % new_offsets
        if execute:
            reqs = [OffsetCommitRequest(topic, p, new_offsets[p], None) for p in partitions]
            resps = client.send_offset_commit_request(group, reqs, fail_on_error=True)
            print 'offsets written.'
    finally:
        client.close()

def expand_partition_num(literal):
    values = []
    for token in literal.split(','):
        if token.find('-') > -1:
            start, end = token.split('-')
            for i in range(int(start), int(end)+1):
                values.append(i)
        else:
            values.append(int(token))
    return values

if __name__ == '__main__':
    args = parse_args()
    enforce_partitions = None
    if len(args.partitions) > 0:
        enforce_partitions = expand_partition_num(args.partitions)

    source_topic_offsets, source_group_offsets = read_consumer_offset(args.source_cluster, args.topic, args.group, enforce_partitions)
    delta = {p: max(0, source_topic_offsets[p] - source_group_offsets[p]) for p in source_group_offsets}
    print 'delta: %s' % delta
    target_group = args.target_group if args.target_group else args.group
    write_consumer_offset_delta(args.target_cluster, args.topic, target_group, delta, args.execute, enforce_partitions)

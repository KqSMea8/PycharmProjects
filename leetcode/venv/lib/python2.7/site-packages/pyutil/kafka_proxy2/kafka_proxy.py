#!/usr/bin/env python
# coding: utf-8
import sys
import time
import os
import logging
import kafka2
import socket
import fcntl
import struct
from kafka2.consumer.fetcher import ConsumerRecord
from kafka2.partitioner.default import DefaultPartitioner
from kafka2.structs import TopicPartition
from pyutil.program.conf import Conf
from pyutil.program import metrics2 as metrics


class KafkaProxy(object):
    def __init__(self, cluster_name, topic, codec='snappy',
                 buffer_size=33554432,
                 consumer_group=None,
                 conf='/opt/tiger/ss_conf/ss/kafka.conf',
                 debug=False, timeout=40,
                 retry_num=3, key_hash=False, partitions=None,
                 req_acks=1, socket_buffer_size=131072,
                 partitioner=DefaultPartitioner(),
                 max_partition_fetch_bytes=1 * 1024 * 1024,
                 is_get_partition_info=False, enable_auto_commit=True, auto_commit_interval=5,
                 default_offset_commit_callback=None,
                 **other_configs):
        '''
        cluster_name:           The kafka cluster name
        topic:                  the topic name
        codec:                  the transmission compression algorithm, snappy by default
        buffer_size:            The total bytes of memory the producer should use to buffer records waiting to be sent to the server.
                                default 32M.
        consumer_group:         A name for this consumer, used for offset storage and must be unique
        conf:                   The configuration file path. '/opt/tiger/ss_conf/ss/kafka.conf' by default
        debug:                  Currently unused
        timeout:                Socket timeout in seconds, it also used by the consumer to
                                indicate that how much time (in seconds) to wait for a message in
                                the iterator before exiting.
        retry_num:              retry times to produce a message.
        key_hash:               whether use the `hash by key` to choose a partition, false by default
        partitions:             An optional list of partitions to consume the data from
        req_acks:               A value indicating the acknowledgements that the server must receive before responding to the request
                                0 means do not wait for the acknowledgements at all
                                1 means only wait for the leader's acknowledgement
                                -1 means wait all isr's acknowledgements
                                1 by default
        socket_buffer_size:     Socket buffer size by default
        is_get_partition_info:  Whether get partition meta information
        partitioner:            A partitioner class that will be used to get the partition to send the message to.
                                Must be derived from Partitioner
        max_partition_fetch_bytes: This size must be at least as large as the maximum message sizethe server allows or else it is
                                possible for the producer to send messages larger than the consumer can fetch. If that happens,
                                the consumer can get stuck trying to fetch a large message on a certain partition. Default: 1048576.
        enable_auto_commit:     If True , the consumer’s offset will be periodically committed in the background.
                                Default: True.

        auto_commit_interval:   Number of seconds between automatic offset commits, if enable_auto_commit is True.
                                Default: 5.
        default_offset_commit_callback: Called as callback(offsets, response) response will be either an Exception or
                                an OffsetCommitResponse struct. This callback can be used to trigger custom actions
                                when a commit request completes.
        other_configs:          kafka consumer or producer configs
        '''
        if cluster_name is None:
            from pyutil.kafka_proxy.kafka_router import get_cluster
            cluster_name = get_cluster(topic)
        self.is_test = os.getenv('TCE_PERF_TEST', None)
        self.original_cluster_name = cluster_name
        self.original_topic = topic
        self.cluster_name = cluster_name
        self.topic = topic
        from pyutil.kafka_proxy.kafka_router import get_redirection
        redirection = get_redirection(self.original_cluster_name, self.original_topic, False)
        if redirection is not None:
            if 'cluster_name' in redirection:
                # remember UTF8 convertion
                self.cluster_name = str(redirection['cluster_name'])
            if 'topic' in redirection:
                self.topic = str(redirection['topic'])
        if self.original_cluster_name != self.cluster_name or self.original_topic != self.topic:
            logging.warning("Using redirection: cluster_name=%s, topic=%s, original_cluster_name=%s, original_topic=%s"
                    % (self.cluster_name, self.topic, self.original_cluster_name, self.original_topic))
        self.codec = codec
        self.buffer_size = buffer_size
        self.consumer_group = consumer_group
        if isinstance(conf, Conf):
            self.conf = conf
        else:
            self.conf = Conf(conf)
        self.debug = debug
        self.timeout = timeout  # 执行超时时间
        self.retry_num = retry_num  # 发送数据失败重试最大次数
        self.key_hash = key_hash
        if partitions == None: #血淋淋的教训， 之前使用if not partitions, 无法正确处理partitions=0的情况
            self.partitions = None
        else:
            if not isinstance(partitions, list):
                self.partitions = []
                self.partitions.append(int(partitions))
            else:
                self.partitions = partitions
        self.req_acks = req_acks
        self.socket_buffer_size = socket_buffer_size
        self.partitioner = partitioner
        self.producer = None
        self.consumer = None
        self.max_partition_fetch_bytes = max_partition_fetch_bytes
        self.is_get_partition_info = is_get_partition_info
        self.enable_auto_commit = enable_auto_commit
        self.auto_commit_interval = auto_commit_interval
        self.broker_list = None
        if timeout is None:
            self.timeout = 40
        if default_offset_commit_callback is None:
            def dummy_callback():
                pass

            self.default_offset_commit_callback = dummy_callback
        else:
            self.default_offset_commit_callback = default_offset_commit_callback
        self.other_configs = other_configs
        self.client_id_suffix = '|kafka-python-new|%s|%s-%s' % (
            self._get_ip_address(), time.strftime('%Y-%m-%d-%H-%M-%S'),
            ("%.9f" % time.time()).split('.')[1])
        self._check_kafka_broker_list()

    def __del__(self):
        self._close_kafka_consumer()
        self._close_kafka_producer()

    def get_kafka_producer(self):
        self._check_kafka_producer()
        return self.producer

    def get_kafka_consumer(self):
        self._check_kafka_consumer()
        return self.consumer

    def _close_kafka_consumer(self):
        if self.consumer is not None:
            self.consumer.close()
            self.consumer = None

    def _close_kafka_producer(self):
        if self.producer is not None:
            self.producer.close(timeout=0)
            self.producer = None

    def _check_kafka_consumer(self):
        self._check_kafka_broker_list()
        if not self.consumer:
            if not self.partitions:
                self.consumer = kafka2.KafkaConsumer(self.topic,
                                                     cluster_name=self.cluster_name,
                                                     client_id=self.consumer_group + self.client_id_suffix,
                                                     group_id=self.consumer_group,
                                                     bootstrap_servers=self.broker_list,
                                                     enable_auto_commit=self.enable_auto_commit,
                                                     auto_commit_interval_ms=self.auto_commit_interval * 1000,
                                                     receive_buffer_bytes=self.socket_buffer_size,
                                                     send_buffer_bytes=self.socket_buffer_size,
                                                     max_partition_fetch_bytes=self.max_partition_fetch_bytes,
                                                     request_timeout_ms=self.timeout * 1000,
                                                     **self.other_configs)
            else:
                self.consumer = kafka2.KafkaConsumer(cluster_name=self.cluster_name,
                                                     client_id=self.consumer_group + self.client_id_suffix,
                                                     group_id=self.consumer_group,
                                                     bootstrap_servers=self.broker_list,
                                                     enable_auto_commit=self.enable_auto_commit,
                                                     auto_commit_interval_ms=self.auto_commit_interval * 1000,
                                                     receive_buffer_bytes=self.socket_buffer_size,
                                                     send_buffer_bytes=self.socket_buffer_size,
                                                     max_partition_fetch_bytes=self.max_partition_fetch_bytes,
                                                     request_timeout_ms=self.timeout * 1000,
                                                     **self.other_configs)
                self.consumer.assign([TopicPartition(self.topic, partition) for partition in self.partitions])
            self.consumer.ensure_partition_assignment()

    def _check_kafka_producer(self):
        self._check_kafka_broker_list()
        if not self.producer:
            acks = 'all' if self.req_acks == -1 else self.req_acks
            self.producer = kafka2.KafkaProducer(bootstrap_servers=self.broker_list,
                                                 cluster_name=self.cluster_name,
                                                 compression_type=self.codec,
                                                 buffer_memory=self.buffer_size,
                                                 client_id='produce-group' + self.client_id_suffix,
                                                 receive_buffer_bytes=self.socket_buffer_size,
                                                 send_buffer_bytes=self.socket_buffer_size,
                                                 partitioner=self.partitioner,
                                                 request_timeout_ms=self.timeout * 1000,
                                                 retries=self.retry_num,
                                                 acks=acks,
                                                 **self.other_configs)

    def _check_kafka_cluster_name(self):
        valid_cluster = self.conf.get_values("valid_cluster")
        if self.cluster_name not in valid_cluster:
            logging.error("not support cluster name %s, please check valid_cluster in conf",
                    self.cluster_name)
            raise ValueError("not support cluster name %s, please check valid_cluster in conf" %
                    self.cluster_name)

    def _check_bootstrap_server(self, broker_list):
        import re
        import socket
        pattern = re.compile(r".*?\[?([0-9a-zA-Z\-%._:]*)\]?:([0-9]+)")
        for host_and_port in broker_list:
            match = pattern.match(host_and_port)
            if match:
                try:
                    host = match.group(1)
                    port = int(match.group(2))
                    ok = False
                    for af in (socket.AF_INET, socket.AF_INET6):
                        try:
                            socket.inet_pton(af, host)
                            ok = True
                            break
                        except (ValueError, AttributeError, socket.error):
                            continue
                    assert ok, 'illegal hostname'
                    assert port >= 0 and port <= 65535, 'port out of range: ' + str(port)
                except Exception, e:
                    return False
            else:
                return False
        return True

    def _check_kafka_broker_list(self):
        self._check_kafka_cluster_name()
        self.broker_list = self.conf.get_values(self.cluster_name)
        if not self.broker_list or not self._check_bootstrap_server(self.broker_list):
            logging.error("no available kafka broker found. Please check Conf or consul.")
            raise ValueError("no available kafka broker found. Please check Conf or consul.")

    @staticmethod
    def _get_ip_address():
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        try:
            ip_address = socket.inet_ntoa(fcntl.ioctl(
                s.fileno(),
                0x8915,  # SIOCGIFADDR
                struct.pack('256s', 'eth0')
            )[20:24])
        except Exception, err:
            logging.exception("get ip address error: %s", err)
            ip_address = "unknown-host"
        finally:
            s.close()
        return ip_address.replace('.', '-')

    def write_msgs(self, msgs, block=False):
        if self.is_test == '1':
            return
        producer = self.get_kafka_producer()
        if not isinstance(msgs, list):
            msgs = [msgs]
        for msg in msgs:
            if self.key_hash: #msgs格式必须是list, [(key, msg), (key, msg)]
                producer.send(self.topic, value=msg[1], key=msg[0])
            else:
                producer.send(self.topic, value=msg)
        if block:
            producer.flush()

    def flush_writen_msgs(self):
        producer = self.get_kafka_producer()
        producer.flush()

    def fetch_msgs_with_offset(self, count, block=True, timeout=0.5):
        msgs = self.__fetch_msgs(count=count, block=block, timeout=timeout)
        if msgs:
            if self.is_get_partition_info:
                return [(msg.offset, msg.value, msg.partition) for msg in msgs]
            else:
                return [(msg.offset, msg.value) for msg in msgs]
        else:
            return None

    def fetch_msgs_with_meta(self, count, block=True, timeout=0.5):
        msgs = self.__fetch_msgs(count=count, block=block, timeout=timeout)
        if msgs:
            if self.is_get_partition_info:
                return [(msg.offset, msg.value, msg.partition, self.topic) for msg in msgs]
            else:
                return [(msg.offset, msg.value, self.topic) for msg in msgs]
        else:
            return None

    def fetch_msgs(self, count=100, block=True, timeout=0.5):
        msgs = self.__fetch_msgs(count=count, block=block, timeout=timeout)
        if msgs:
            if self.is_get_partition_info:
                return [(msg.value, msg.partition) for msg in msgs]
            else:
                return [msg.value for msg in msgs]
        else:
            return None

    def __fetch_msgs(self, count=100, block=True, timeout=0.5):
        if not block:
            timeout = 0
        consumer = self.get_kafka_consumer()
        msg_list = []
        msg_map = consumer.poll(timeout_ms=timeout * 1000, max_records=count)
        for msgs in msg_map.values():
            msg_list.extend(msgs)
        return msg_list

    def commit(self):
        consumer = self.get_kafka_consumer()
        return consumer.commit()

    def commit_async(self):
        consumer = self.get_kafka_consumer()
        return consumer.commit_async()

    def get_partitions(self):
        consumer = self.get_kafka_consumer()
        return set([partition.partition for partition in consumer.assignment()])

    def set_consumer_offset(self, offset, whence, force=False):
        consumer = self.get_kafka_consumer()
        if whence == 0:
            consumer.seek_to_beginning_with_offsets(relative_offsets=offset)
        elif whence == 1:
            for partition in consumer.assignment():
                offset_now = consumer.committed(partition)
                if offset_now is not None:
                    consumer.seek(partition, offset_now + offset)
                else:
                    # 没获取到该group的offset,跳到最新的位置
                    consumer.seek_to_end_with_offsets(partitions=[partition], relative_offsets=offset)
        elif whence == 2:
            consumer.seek_to_end_with_offsets(relative_offsets=offset)
        else:
            raise ValueError("whence should be 0,1,2")
        if force:
            consumer.update_positions()
            consumer.commit()

    def get_consumer_offset(self):
        consumer = self.get_kafka_consumer()
        offsets = {}
        for partition in consumer.assignment():
            offset = consumer.committed(partition)
            offsets[partition.partition] = offset
        return offsets

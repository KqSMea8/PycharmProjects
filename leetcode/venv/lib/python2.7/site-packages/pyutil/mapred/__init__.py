#!/usr/bin/env python
#coding=utf8
import os, pdb, sys, logging
import gc

class MapReduceJob(object):
    hadoop_path = "/opt/tiger/hadoop_deploy/hadoop-1.0.3/bin/"

    hadoop2_home = "/opt/tiger/yarn_deploy/hadoop"
    hadoop2_bin = hadoop2_home+"/bin/hadoop"
    hadoop2_streaming = hadoop2_home+"/share/hadoop/tools/lib/hadoop-streaming-2.6.0-cdh5.4.4.jar"

    hadooplf_home = "/opt/tiger/yarn_deploy/hadoop-2.6.0-cdh5.4.4"
    hadooplf_bin = hadooplf_home+"/bin/hadoop"
    hadooplf_streaming = hadooplf_home+"/share/hadoop/tools/lib/hadoop-streaming-2.6.0-cdh5.4.4.jar"

    def __init__(self, log_path='/dev/null', job_name=''):
        self.log_path = log_path
        self.job_name = job_name if job_name else self.__class__.__name__
        self.this_file = self.this_file_name()
        self.counters = dict ()
        self.reducer_num = 0

    def this_file_name(self):
        return __file__

    def inc_counter (self, counter_name, inc_num = 1):
        if counter_name in self.counters:
            self.counters[counter_name] += inc_num
        else:
            self.counters[counter_name] = inc_num

    def _print_counters(self):
        for k, v in self.counters.items():
            print >> sys.stderr, 'reporter:counter:%s,%s,%d' % ("user_counters", k, v)

    def map(self):
        map_cnt = 0
        while True:
            line = sys.stdin.readline()
            if not line:
                break
            self.mapper(line.strip())
            map_cnt += 1
            if map_cnt % 10000 == 0:
                gc.collect()
        self.map_end()
        self._print_counters()

    def map_end(self):
        pass

    def reduce(self):
        self._reduce(self.reducer)

    def reduce_end(self):
        pass

    def combine (self):
        self._reduce(self.combiner)

    def _reduce(self, key_processor):
        cur_key = None
        values = []
        reduce_cnt = 0

        while True:
            line = sys.stdin.readline()
            if not line:
                if cur_key:
                    key_processor(cur_key, values)
                break

            tokens = line.strip().split('\t')
            if len(tokens) < 2:
                self.inc_counter("reduce_split_err")
                continue
            key, value = tokens[0], tokens[1]
            if key != cur_key:
                if cur_key:
                    key_processor(cur_key, values)
                    reduce_cnt += 1
                    del values

                cur_key = key
                if reduce_cnt % 10000 == 0:
                    gc.collect()

                values = []

            values.append(value)
        self.reduce_end()
        self._print_counters()


    def execute(self, cmd):
        log_dir = os.path.dirname(self.log_path)
        if not os.path.exists (log_dir):
            os.mkdir (log_dir)

        cmds = cmd.split(';')
        cmd_ = ';\n'.join(['%s >> %s 2>&1' % (c, self.log_path) for c in cmds])
        logging.info('execute_cmd: >>>>>>>>>>>>\n%s\n', cmd_)
        ret = os.system(cmd_)
        return ret, cmd_


    def streaming(self, inputs, output, mapper_cmd, reducer_cmd='', combiner_cmd='', files=[], opts=""):
        input_files = ' '.join(['-input %s' % i for i in inputs])
        output_file = '-output %s' % output
        files = ' '.join(['-file %s' % i for i in files])
        if reducer_cmd:
            reducer_cmd = '-reducer "%s"' % reducer_cmd
            if self.reducer_num > 0:
                opts += ' -Dmapred.reduce.tasks=%s' % self.reducer_num
        else:
            opts += ' -Dmapred.reduce.tasks=0'

        if combiner_cmd:
            combiner_cmd = '-combiner "%s"' % combiner_cmd

        remove_output_cmd ="%shadoop fs -rmr %s" %(self.hadoop_path, output_file.split()[1])
        self.execute(remove_output_cmd)
        cmd = ('%s/hadoop jar %s/../contrib/streaming/hadoop-streaming-1.0.3.jar ' +
               '-Dmapred.job.name="%s" %s ' +
               '-mapper "%s" %s %s %s %s %s -lazyOutput') % (
               self.hadoop_path, self.hadoop_path, self.job_name, opts,
               mapper_cmd, reducer_cmd, combiner_cmd,
               input_files, output_file, files)
        ret, cmd_ = self.execute(cmd)
        print "streaming=%s" % cmd_
        return ret, cmd_

    def streaming2(self, inputs, output, mapper_cmd, reducer_cmd='', combiner_cmd='', files=[], queue='offline.data', opts=""):
        input_files = ' '.join(['-input %s' % i for i in inputs])
        output_file = '-output %s' % output
        files = ' '.join(['-file %s' % i for i in files])
        if reducer_cmd:
            reducer_cmd = '-reducer "%s"' % reducer_cmd
        else:
            opts += ' -Dmapred.reduce.tasks=0'

        if combiner_cmd:
            combiner_cmd = '-combiner "%s"' % combiner_cmd

        remove_output_cmd ="%s fs -rmr %s" %(self.hadoop2_bin, output_file.split()[1])
        cmd = ('%s jar %s ' +
               '-Dmapred.job.name="%s" -Dmapred.job.queue.name="%s"  %s ' +
               '-mapper "%s" %s %s %s %s %s') % (
               self.hadoop2_bin, self.hadoop2_streaming, self.job_name, queue, opts,
               mapper_cmd, reducer_cmd, combiner_cmd,
               input_files, output_file, files)

        self.execute(remove_output_cmd)

        ret, cmd_ = self.execute(cmd)
        print "streaming=%s" % cmd_
        return ret, cmd_

    def streaming3(self, inputs, output, mapper_cmd, reducer_cmd='', combiner_cmd='', files=[], queue='offline.data', opts=""):
        input_files = ' '.join(['-input %s' % i for i in inputs])
        output_file = '-output %s' % output
        files = ' '.join(['-file %s' % i for i in files])
        if reducer_cmd:
            reducer_cmd = '-reducer "%s"' % reducer_cmd
        else:
            opts += ' -Dmapred.reduce.tasks=0'

        if combiner_cmd:
            combiner_cmd = '-combiner "%s"' % combiner_cmd

        remove_output_cmd ="%s fs -rmr %s" %(self.hadoop2_bin, output_file.split()[1])
        cmd = ('/opt/tiger/ss_lib/bin/mr ' +
               '-Dmapred.job.name="%s" -Dmapred.job.queue.name="%s"  %s ' +
               '-mapper "%s" %s %s %s %s %s') % (
               self.job_name, queue, opts,
               mapper_cmd, reducer_cmd, combiner_cmd,
               input_files, output_file, files)

        self.execute(remove_output_cmd)

        ret, cmd_ = self.execute(cmd)
        print "streaming=%s" % cmd_
        return ret, cmd_

    def streaming2auto(self, inputs, output, mapper_cmd, reducer_cmd='', combiner_cmd='', files=[], queue='offline.data', opts=""):
        input_files = ' '.join(['-input %s' % i for i in inputs])
        output_file = '-output %s' % output
        files = ' '.join(['-file %s' % i for i in files])
        if reducer_cmd:
            reducer_cmd = '-reducer "%s"' % reducer_cmd
        else:
            opts += ' -Dmapred.reduce.tasks=0'

        if combiner_cmd:
            combiner_cmd = '-combiner "%s"' % combiner_cmd

        remove_output_cmd ="%s fs -rmr %s" %(self.hadooplf_bin, output_file.split()[1])
        cmd = ('/opt/tiger/ss_lib/bin/mr ' +
               '-Dmapred.job.name="%s" -Dmapred.job.queue.name="%s"  %s ' +
               '-mapper "%s" %s %s %s %s %s') % (
               self.job_name, queue, opts,
               mapper_cmd, reducer_cmd, combiner_cmd,
               input_files, output_file, files)

        self.execute(remove_output_cmd)

        ret, cmd_ = self.execute(cmd)
        print "streaming=%s" % cmd_
        return ret, cmd_

    def streaminglf(self, inputs, output, mapper_cmd, reducer_cmd='', combiner_cmd='', files=[], queue='offline.data', opts=""):
        input_files = ' '.join(['-input %s' % i for i in inputs])
        output_file = '-output %s' % output
        files = ' '.join(['-file %s' % i for i in files])
        if reducer_cmd:
            reducer_cmd = '-reducer "%s"' % reducer_cmd
        else:
            opts += ' -Dmapred.reduce.tasks=0'

        if combiner_cmd:
            combiner_cmd = '-combiner "%s"' % combiner_cmd

        remove_output_cmd ="%s fs -rmr %s" %(self.hadooplf_bin, output_file.split()[1])
        cmd = ('%s jar %s ' +
               '-Dmapred.job.name="%s" -Dmapred.job.queue.name="%s"  %s ' +
               '-mapper "%s" %s %s %s %s %s') % (
               self.hadooplf_bin, self.hadooplf_streaming, self.job_name, queue, opts,
               mapper_cmd, reducer_cmd, combiner_cmd,
               input_files, output_file, files)

        self.execute(remove_output_cmd)

        ret, cmd_ = self.execute(cmd)
        print "streaming=%s" % cmd_
        return ret, cmd_

    def simple_run(self, inputs, output, step_num):
        '''
        assuming run the job exactly in the directory of the python file
        step_num specifies how many steps should the job do
        '''
        print "in simple_run"
        mapper_cmd = reducer_cmd = combiner_cmd = ""
        if step_num >= 1:
            mapper_cmd = "%s -m" % self.this_file
        if step_num >= 2:
            reducer_cmd = "%s -r" % self.this_file
        if step_num == 3:
            combiner_cmd = "%s -c" % self.this_file

        files = [self.this_file]
        opts=" -Dmapred.min.split.size=768000000 -Dmapred.reduce.tasks=256 "
        return self.streaming(inputs, output, mapper_cmd, reducer_cmd, combiner_cmd, files, opts)


    def run (self, argv):
        '''
        for those jobs only need to give output, inputs, and run steps number
        step_num=1: map only job
        step_num=2: map-reduce job
        step_num=3: map-combiner-reduce job
        eg, this_job.py step_num output input0 input1 input2...
        can be direct invoked by main
        @param argv: sys.argv
        @param job: a instance of MapReduceJob
        '''
        if len(argv) >= 4:
            print "argv=%s" % argv
            step_num = int(argv[1])
            output = argv[2]
            inputs = argv[3:]
            ret, cmd = self.simple_run(inputs, output, step_num)
            print "run job %s! cmd=%s" % ("OK" if ret == 0 else "ERROR",cmd)

        if len(argv) == 2 and argv[1] == "-m":
            self.map()
        if len(argv) == 2 and argv[1] == "-r":
            self.reduce()
        if len(argv) == 2 and argv[1] == "-c":
            self.combine()

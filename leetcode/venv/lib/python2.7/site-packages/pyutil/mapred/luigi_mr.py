# -*- coding: utf-8 -*-
#
# Copyright 2012-2015 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
import ujson, sys, logging
import functools, Queue, threading, luigi
from luigi.contrib.hadoop import *
from luigi.contrib.hdfs.target import HdfsTarget
from operator import itemgetter
import pystream_runner
from collections import defaultdict

logger = logging.getLogger('luigi-interface')
HADOOP_BIN='/opt/tiger/yarn_deploy/hadoop/bin/hadoop'

def check_scheduler_args():
    if sys.argv is None or not sys.argv:
        return
    if '--local-test' in sys.argv or '--local-simulate' in sys.argv:
        sys.argv.append('--local-scheduler')

def format_date(dt, delimiter=False):
    if delimiter:
        fmt = '%Y-%m-%d'
    else:
        fmt = '%Y%m%d'
    return dt.strftime(fmt)

def generate_nothing(f):
    @functools.wraps(f)
    def wrapper(*args,**kw):
        f(*args,**kw)
        if False:
            yield None
    return wrapper

def get_writable_class_by_type(t):
    if t == str or t == unicode:
        return 'org.apache.hadoop.io.Text'
    if t == long or t == int:
        return 'org.apache.hadoop.io.LongWritable'
    if t == float:
        return 'org.apache.hadoop.io.DoubleWritable'
    if t == None or t == type(None):
        return 'org.apache.hadoop.io.NullWritable'
    return 'org.apache.hadoop.io.BytesWritable'

HADOOP_HOME = '/opt/tiger/yarn_deploy/hadoop'
PYSTREAM_JAR = '%s/share/hadoop/tools/lib/pystream-0.0.1-SNAPSHOT.jar' % HADOOP_HOME

# do this before 'luigi.run'
check_scheduler_args()

class YarnMapReduceRunner(HadoopJobRunner):

    def __init__(self, owner, use_native):
        self.use_native = use_native
        super(YarnMapReduceRunner, self).__init__(streaming_jar=PYSTREAM_JAR)
        if owner is None:
            raise Exception('owner must be specified')
        self.init_pystream_conf(self.jobconfs)

    def init_pystream_conf(self, jobconfs):
        self.jobconfs['pystream.mapper.source'] = 'pystream_runner.py map'
        self.jobconfs['pystream.reducer.source'] = 'pystream_runner.py reduce'
        self.jobconfs['pystream.combiner.source'] = 'pystream_runner.py combiner'
        self.jobconfs['pystream.final.function'] = 'final'
        self.jobconfs['pystream.local.out'] = 'stderr'
        if not hasattr(self, 'libjars') or self.libjars is None or len(self.libjars) == 0:
            self.libjars = filter(lambda x: x, luigi.configuration.get_config().get('hadoop', 'libjars', '').split(','))
        self.libjars.append('/opt/tiger/yarn_deploy/hadoop/bytedance-data-1.0.1.jar')
        if self.use_native:
            self.jobconfs['native.platforms'] = 'org.apache.hadoop.mapred.nativetask.HadoopPlatform,org.apache.hadoop.streaming.StreamingPlatform'
            self.jobconfs['mapreduce.job.map.output.collector.class'] = 'org.apache.hadoop.mapred.nativetask.NativeMapOutputCollectorDelegator'

    def init_types(self, job, jobconfs):
        jobconfs['pystream.map.output.key.class'] = get_writable_class_by_type(job.map_output_key_type)
        jobconfs['pystream.map.output.value.class'] = get_writable_class_by_type(job.map_output_value_type)

    def run_job(self, job, tracking_url_callback=None):
        luigi.contrib.hadoop.mrrunner = pystream_runner
        self.jobconfs['pystream.mapper.value.deserializer'] = job.input_deserializer
        self.init_types(job, self.jobconfs)
        return super(YarnMapReduceRunner, self).run_job(job, tracking_url_callback)


class LocalTestJobRunner(YarnMapReduceRunner):

    def __init__(self):
        super(LocalTestJobRunner, self).__init__(owner='test', use_native=False)
        self.jobconfs['pystream.mode'] = 'localTest'
        self.end_job_with_atomic_move_dir = False


class LocalSimulateJobRunner(YarnMapReduceRunner):
    libjars = []
    jobconfs = {}

    def __init__(self, seed, records, records_per_file, records_out):
        super(LocalSimulateJobRunner, self).__init__(owner='test', use_native=False)
        self.jobconfs['pystream.mode'] = 'localSimulate'
        self.jobconfs['pystream.seed'] = seed
        self.jobconfs['pystream.simulate.records'] = records
        self.jobconfs['pystream.simulate.records-per-file'] = records_per_file
        self.jobconfs['pystream.simulate.output'] = records_out
        self.end_job_with_atomic_move_dir = False

@generate_nothing
def noop():
    pass

def test_requires():
    return []

def test_complete():
    return False

def extract_target_path(output):
    if isinstance(output, list):
        return filter(lambda x: x is not None, map(lambda x: extract_target_path(x)[0], output))
    elif isinstance(output, HdfsTarget):
        return [output.path]
    else:
        return [None]

class IndependentMetaJobTask(luigi.task.Register):

    def __call__(cls, *args, **kwargs):
        instance = super(IndependentMetaJobTask, cls).__call__(*args, **kwargs)
        if instance.local_test:
            instance.requires = test_requires
        if instance.local_test or instance.local_simulate:
            instance.complete = test_complete
        return instance


class PystreamMapReduceTask(JobTask):
    __metaclass__ = IndependentMetaJobTask

    # essentials
    owner = luigi.parameter.Parameter(significant=True)

    # test & simulations
    rerun = luigi.parameter.BoolParameter(default=False, significant=False)
    local_test = luigi.parameter.BoolParameter(default=False, significant=False)
    local_simulate = luigi.parameter.BoolParameter(default=False, significant=False)
    simulate_num_records = luigi.parameter.IntParameter(default=1000, significant=False)
    simulate_num_records_per_file = luigi.parameter.IntParameter(default=100, significant=False)
    simulate_records_out = luigi.parameter.Parameter(default='/dev/null', significant=False)

    # (de)optimizations
    use_native = luigi.parameter.BoolParameter(default=True, significant=False)
    input_deserializer = luigi.parameter.Parameter(default='plain', significant=False)

    # runtime profiling
    profiling = luigi.parameter.BoolParameter(default=False, significant=False)
    profiling_ratio = luigi.parameter.IntParameter(default=100, significant=False)

    map_output_key_type = str
    map_output_value_type = str

    def complete(self):
        is_complete = super(PystreamMapReduceTask, self).complete()
        if is_complete:
            if self.rerun:
                logger.warn("Output directory exists. Removing because '--rerun' is specified.")
                targets = extract_target_path(self.output())
                for target in targets:
                    HdfsTarget(target).remove(skip_trash=False)
                return False
            else:
                rm_cmd = ' '.join(extract_target_path(self.output()))
                logger.warn("Output directory exists! If rerun is desired, execute 'hadoop fs -rmr %s' or run task with '--rerun'." % rm_cmd)
        return is_complete

    @property
    def owner_email(self):
        if self.owner is None:
            return None
        owners = self.owner.split(',') if isinstance(self.owner, str) else self.owner
        return ','.join(map(lambda x: x+'@bytedance.com', owners))

    # should override this for map input file awareness
    def update_jobconf(self, jc):
        self.jobconf = jc

    def gather_counters(self):
        result = {}
        for group in self.counters:
            for name in self.counters[group]:
                key = '%s#%s' % (group, name)
                result[key] = self.counters[group][name]
        return result

    def incr_counter(self, group, name, delta):
        self.counters[group][name] += delta

    def job_runner(self):
        if self.owner is None:
            raise Exception('owner not specified')
        if self.local_test:
            return LocalTestJobRunner()
        elif self.local_simulate:
            return LocalSimulateJobRunner(1, self.simulate_num_records, self.simulate_num_records_per_file, self.simulate_records_out)
        else:
            return YarnMapReduceRunner(self.owner, self.use_native)

    def run(self, tracking_url_callback=None):
        self.init_local()
        try:
            self.job_runner().run_job(self, tracking_url_callback=tracking_url_callback)
        except TypeError as ex:
            if 'unexpected keyword argument' not in ex.message:
                raise
            self.job_runner().run_job(self)

    def should_profile(self):
        if not self.profiling:
            return False
        import random
        r = random.randint(0, 99)
        if r <= self.profiling_ratio:
            return True
        return False

    def init_counters(self):
        self.counters = defaultdict(lambda: defaultdict(lambda: 0))

    def run_mapper(self, stdin=sys.stdin, stdout=sys.stdout):
        self.init_counters()
        self.init_hadoop()
        self.init_mapper()
        mapper = self.mapper
        final = noop if self.final_mapper == NotImplemented else self.final_mapper
        return mapper, final, self.update_jobconf, self.gather_counters

    def run_reducer(self, stdin=sys.stdin, stdout=sys.stdout):
        self.init_counters()
        self.init_hadoop()
        self.init_reducer()
        reducer = self.reducer
        final = noop if self.final_reducer == NotImplemented else self.final_reducer
        return reducer, final, self.update_jobconf, self.gather_counters

    def run_combiner(self, stdin=sys.stdin, stdout=sys.stdout):
        self.init_counters()
        self.init_hadoop()
        self.init_combiner()
        combiner = self.combiner
        final = noop if self.final_combiner == NotImplemented else self.final_combiner
        return combiner, final, self.update_jobconf, self.gather_counters

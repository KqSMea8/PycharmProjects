#!/usr/bin/env python
# coding=utf8

"""
map reduce 的框架类, 主要完成如下几个事情：
    1. 在 reducer 中将同一个 key 的 value 合并到一起，不用程序员自己再判断 lastkey 之类的了
    2. 集成 counter，可以做些自定义的计数
    3. 设置脚本运行环境，包括 hadoop 的位置，java_library_path 等

使用方法:

1. 继承实现 MapReducer

class MyMapReducer(MapReducer):

    def mapper(self, line):
        # blabla

    def reduce(self, key, value_iter):
        for value in value_iter:
            # blabla

2. 继承 MRJob

class MyMRJob(MRJob):
    inputs = ['inputs']
    output = 'output'
    job_name = 'job_name'
    file_name = __file__

    def get_mapper_reducer(self):
        return MyMapReducer()

3. 调用

if __name__ == '__main__':
    job = MyMRJob()
    job.run()

--------------- 我是分割线 -------------------
注意：
1. MapReducer 中的 counter 定义不要超过 16 个，否则 hadoop 会报错
2. 可以实现 MapReducer 中的 map_end 和 reduce_end, 以做一些整体的统计等工作
3. MRJob 中的 extra_flags 等可以自定义 hadoop 的一些参数

"""

__author__ = 'jiangfeiyun'

import sys
from collections import Counter, Iterator


class ValueIterator(Iterator):
    """担心同一个 key 的 value 太多，内存装不下，因此这里使用 Iterator 的方式
    """

    def __init__(self, key, value):
        super(ValueIterator, self).__init__()
        self.last_key = key
        self.last_value = value
        self.count = -1
        self.next_key = None
        self.next_value = None
        self.end = False

    def next(self):
        self.count += 1
        if self.count == 0:
            return self.last_value
        else:
            if self.end:
                raise StopIteration
            while 1:
                line = sys.stdin.readline()
                if not line:
                    self.end = True
                    raise StopIteration
                items = line.strip().split('\t')
                if len(items) != 2:
                    continue
                key, value = items
                if key != self.last_key:
                    self.next_key = key
                    self.next_value = value
                    self.end = True
                    raise StopIteration
                else:
                    return value

    def iter_to_end(self):
        while not self.end:
            try:
                self.next()
            except StopIteration:
                break


class MapReducer(object):

    def __init__(self):
        super(MapReducer, self).__init__()
        self.counters = Counter()

    def map_end(self):
        pass

    def mapper(self, line):
        pass

    def do_map(self):
        for line in sys.stdin:
            self.mapper(line)
        self.map_end()
        self._submit_counters()

    def reduce_end(self):
        pass

    def reduce(self, key, value_iter):
        pass

    def do_reduce(self):
        value_iter = None
        while 1:
            line = sys.stdin.readline()
            if not line:
                break
            items = line.strip().split('\t')
            if len(items) != 2:
                continue
            value_iter = ValueIterator(items[0], items[1])
            break

        while value_iter:
            self.reduce(value_iter.last_key, value_iter)
            value_iter.iter_to_end()  # 防止在 reduce 中 break，没把 value_iter 消费完毕
            key, value = value_iter.next_key, value_iter.next_value
            if not key:
                break
            value_iter = ValueIterator(key, value)

        self.reduce_end()
        self._submit_counters()

    def inc_counter(self, counter_name, inc_num=1):
        self.counters[counter_name] += inc_num

    def _submit_counters(self):
        for k, v in self.counters.iteritems():
            print >> sys.stderr, 'reporter:counter:%s,%s,%d' % ("user_counters", k, v)


class MRJob(object):
    """
    定义 Job，需要做的：
    1. 设置 inputs 和 output
    2. 设置 jobname
    3. 设置 file_name
    4. 实现 get_mapper_reducer
    """

    HADOOP_HOME = '/opt/tiger/yarn_deploy/hadoop-2.3.0-cdh5.1.0'
    STREAMING = '%s/share/hadoop/tools/lib/hadoop-streaming-2.3.0-cdh5.1.0.jar' % HADOOP_HOME

    inputs = []
    output = None
    job_name = None
    queue_name = 'tiger'

    extra_flags = ''

    reduce_num = 256
    file_name = ''
    extra_files = []
    logfile = 'stdout'

    def __init__(self):
        self.user_counters = {}
        self.use_mr = False

    def get_mapper_reducer(self):
        return None

    def gen_cmd(self):
        if not self.inputs or not self.output or not self.job_name or not self.file_name:
            print >> sys.stderr, 'no (input|output|jobname|file_name) set'
            sys.exit(1)
        if self.use_mr:
            cmd = ' /opt/tiger/ss_lib/bin/mr'
        else:
            cmd = ' %s/bin/hadoop jar %s' % (self.HADOOP_HOME, self.STREAMING)
        cmd += ' -files %s' % ','.join(self.extra_files + [self.file_name])
        cmd += ' -Dmapred.job.queue.name=%s' % self.queue_name
        cmd += ' -Dmapred.job.name=%s' % self.job_name
        cmd += ' ' + self.extra_flags
        cmd += ' -Dmapred.reduce.tasks=%s' % self.reduce_num
        cmd += ' -output %s' % self.output
        cmd += ' -mapper "python %s -m"' % self.file_name
        cmd += ' -reducer "python %s -r"' % self.file_name
        for i in self.inputs:
            cmd += ' -input %s' % i
        return cmd

    def get_user_counters(self, contents):
        begin = False
        result = {}
        for line in contents:
            if not begin and line.strip() == 'user_counters':
                begin = True
            elif begin and line.startswith('\t\t'):
                if line.find('=') > 0:
                    name, value = line.strip().split('=')
                    result[name] = value
            elif begin:
                break
        return result

    def run(self):
        map_reducer = self.get_mapper_reducer()
        if not map_reducer:
            print >> sys.stderr, 'please implement get_mapper_reducer'
            sys.exit(1)

        if len(sys.argv) > 1 and sys.argv[1] == '-m':
            map_reducer.do_map()
        elif len(sys.argv) > 1 and sys.argv[1] == '-r':
            map_reducer.do_reduce()
        else:
            import os
            import subprocess
            cmd = self.gen_cmd()
            env = 'export JAVA_HOME=/opt/tiger/yarn_deploy/jdk'
            env += ' && export HADOOP_HOME=%s' % self.HADOOP_HOME
            env += ' && export JAVA_LIBRARY_PATH=${HADOOP_HOME}/lib/native:$JAVA_LIBRARY_PATH'
            env += ' && export LD_LIBRARY_PATH=${HADOOP_HOME}/lib/native:$LD_LIBRARY_PATH'
            env += ' && export HADOOP_USER_NAME=tiger'

            os.system(env + ' && %s/bin/hadoop fs -rm -r %s' % (self.HADOOP_HOME, self.output))

            cmd = env + ' && ' + cmd
            print cmd
            process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True)
            out = process.stdout

            if self.logfile == 'stdout':
                fp = sys.stdout
            else:
                fp = open(self.logfile, 'a')

            while 1:
                line = out.readline()
                if not line:
                    break
                print >>fp, line.rstrip()

            rt = process.wait()

            if rt is not None and rt != 0:
                sys.exit(rt)

class LFMRJob(MRJob):
    HADOOP_HOME = '/opt/tiger/yarn_deploy/hadoop-2.6.0-cdh5.4.4'
    STREAMING = HADOOP_HOME + '/share/hadoop/tools/lib/hadoop-streaming-2.6.0-cdh5.4.4.jar'
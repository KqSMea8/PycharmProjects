import logging
import time
import random
from pyutil.memcache.jenkins import oneatatime
from pyutil.net.get_local_ip import get_local_ip
from pyutil.pps.push_kafka_proxy import get_push_kafka_proxy
import pyutil.pps.push_task_pb2 as pps_pb
from pyutil.program import metrics2 as metrics

# TODO add calc by product, add params
def calc_user_shard_by_did(did, total_shards):
    return oneatatime(str(did)) % total_shards

class UserDataWriter(object):
    def __init__(self, kafka_topic, kafka_cluster='kafka_push_lf',
                 buffer_size=3000, buffer_time=600, parts=-1, metrics_sample = 0.01, from_job_name = "no_define"):
        self.kafka_proxy = get_push_kafka_proxy(conf=None, cluster_name=kafka_cluster,
                                                topic=kafka_topic)
        self.buffer_size = buffer_size
        self.buffer_time = buffer_time
        self.last_flush_time = time.time()
        self.msg_buffer = []
        self.msg_ud = []
        self.parts = parts
        self.shard_batch_ud = {}
        self.shard_last_flush_time = {}
        self.local_ip = get_local_ip()
        self.from_job_name = from_job_name

        self.metrics_sample = metrics_sample
        self.metrics_prefix = 'data.push.pps.kafka.' + kafka_cluster + '.' + kafka_topic
        self.pb_field2count = {"profile": 0, "device_info": 0, "actions": 0, "location": 0, "history": 0, "ies_user_feature": 0, "user_candidates": 0, "installed_app": 0, "user_time_stat": 0, "old_profile": 0, "old_device_info": 0, "old_location": 0, "ab_profile": 0, "dedup_filter": 0, "create_if_exist": 0, "msg_create_time": 0, "msg_update_time": 0}

        metrics.define_counter('send.msg', prefix = self.metrics_prefix)
        metrics.define_counter('send.user_data', prefix = self.metrics_prefix)
        metrics.define_counter('send.field', prefix = self.metrics_prefix)
        metrics.define_tagkv("field", self.pb_field2count.keys())
        metrics.define_tagkv("msg_group", ["single", "batch"])
        metrics.define_tagkv("from_job_name", [self.from_job_name])

    def write(self, did, data, is_raw=False):
        '''data is pb object'''
        self.buffered_write(did, data, is_raw)
        self.flush()

    def write_batch(self, did, ud, flush_count=10, identity=''):
        '''ud i.e. user_data is pb object'''
        did = long(did)
        if not did:
            logging.error('did is empty')
            return
        if self.parts <= 0:
            logging.error('unknown partitions num')
            return
        ud.did = did
        shard = calc_user_shard_by_did(did, self.parts)
        if shard not in self.shard_batch_ud:
            bud = pps_pb.BatchUserData()
            bud.batch_id = shard
            bud.write_ip = self.local_ip
            bud.write_identity = identity
            self.shard_batch_ud[shard] = bud
            self.shard_last_flush_time[shard] = time.time()
        self.shard_batch_ud[shard].user_data_list.extend([ud])
        now = time.time()
        if len(self.shard_batch_ud[shard].user_data_list) >= flush_count \
                or (now - self.shard_last_flush_time.get(shard, now)) > self.buffer_time:
            self.flush_batch(shard, self.shard_batch_ud[shard])
            self.shard_last_flush_time[shard] = now

    def flush_batch(self, shard, bud):
        if len(bud.user_data_list) == 0:
            return

        bud.write_time = int(time.time())
        raw = bud.SerializeToString()

        try:
            did = bud.user_data_list[0].did
            self.kafka_proxy.write_msgs([(str(did), raw)])
            if random.random() < self.metrics_sample:
                user_data_count = len(bud.user_data_list)
                metrics.emit_counter("send.msg", 1, tagkv = {"msg_group": "batch", "from_job_name" : self.from_job_name}, prefix = self.metrics_prefix)
                metrics.emit_counter("send.user_data", user_data_count, tagkv = {"msg_group": "batch", "from_job_name" : self.from_job_name}, prefix = self.metrics_prefix)
                for unit in bud.user_data_list:
                    for field_name in self.pb_field2count.keys():
                        if unit.HasField(field_name):
                            self.pb_field2count[field_name] += 1
                for field_name in self.pb_field2count.keys():
                    metrics.emit_counter("send.field", self.pb_field2count[field_name], tagkv = {"msg_group": "batch", "field": field_name, "from_job_name": self.from_job_name}, prefix = self.metrics_prefix)
                    self.pb_field2count[field_name] = 0
        except:
            logging.exception('UserDataWriter write failed')

        del self.shard_batch_ud[shard]

    def flush_all_batch(self):
        for shard, bud in self.shard_batch_ud.items():
            self.flush_batch(shard, bud)

    def buffered_write(self, did, data, is_raw=False):
        '''data is pb object or raw'''
        ud = pps_pb.UserData()
        did = long(did)
        if not did:
            logging.error('did is empty')
            return
        if not is_raw:
            data.did = did # double set
            ud = data
            data = data.SerializeToString()
        else:
            ud.ParseFromString(data)
        self.msg_buffer.append((str(did), data))
        self.msg_ud.append(ud)
        now = time.time()
        if len(self.msg_buffer) >= self.buffer_size or (now - self.last_flush_time) >= self.buffer_time:
            self.last_flush_time = now
            self.flush()

    def flush(self):
        '''flush buffered data'''
        try:
            self.kafka_proxy.write_msgs(self.msg_buffer)
            if random.random() < self.metrics_sample:
                user_data_count = len(self.msg_buffer)
                metrics.emit_counter("send.msg", 1, tagkv = {"msg_group": "single", "from_job_name": self.from_job_name}, prefix = self.metrics_prefix)
                metrics.emit_counter("send.user_data", user_data_count, tagkv = {"msg_group": "single", "from_job_name": self.from_job_name}, prefix = self.metrics_prefix)
                for unit in self.msg_ud:
                    for field_name in self.pb_field2count.keys():
                        if unit.HasField(field_name):
                            self.pb_field2count[field_name] += 1
                for field_name in self.pb_field2count.keys():
                    metrics.emit_counter("send.field", self.pb_field2count[field_name], tagkv = {"msg_group": "single", "field": field_name, "from_job_name": self.from_job_name}, prefix = self.metrics_prefix)
                    self.pb_field2count[field_name] = 0
            self.msg_buffer = []
            self.msg_ud = []
        except:
            logging.exception('UserDataWriter write failed')


#if __name__ == "__main__":
#    from pyutil.pps import push_task_pb2 as pps_pb
#    from app import get_user_data_writer
#
#    metrics.init({'metrics_namespace_prefix' : 'data.push.pps.kafka.test'})
#    ud = pps_pb.UserData()
#    ud.did = 39134193104
#    ud.device_info.active_time = 1517993488
#
#    a = UserDataWriter('strong_push_features', parts = 2048)
#    for i in range(10000):
#        a.write_batch(39134193104, ud)
#        a.buffered_write(39134193104, ud)
#    a.flush_all_batch()
#    a.flush()

#!/usr/bin/env python
# -*- coding : utf8 -*-
import time
import sys
from itertools import chain, starmap
from redis import StrictRedis
from redis.exceptions import ConnectionError,TimeoutError,ResponseError,RedisError
from redis.client import StrictPipeline,list_or_args
from cache_metrics import CacheMetrics, NullMetrics
from redis_connection import ClusterPool,MultiServConnectionPool
from conf_discover import get_psm
import logging

_METRIC_PREFIX = "redis.client"

class RedisClient(StrictRedis):
    '''
    '''
    version = 1
    def __init__(self, cluster, psm = None, servers = None, retry = 1, enable_metrics=True, **kwargs):
        slowlogging = 0
        if  'slowlogging' in kwargs:
            slowlogging = kwargs['slowlogging']
            kwargs.pop('slowlogging')
        psm = get_psm(psm)
        pool = ClusterPool.get_pool(cluster, servers, **kwargs)
        StrictRedis.__init__(self, connection_pool=pool,**kwargs)
        self.cluster = cluster
        self.psm = psm
        self.trycount = 1
        self.retry = retry
        if retry > 0:
            self.trycount += retry
        if enable_metrics:
            self.metrics = CacheMetrics(cluster, _METRIC_PREFIX, psm)
        else:
            self.metrics = NullMetrics()
        self.slowlogging = slowlogging

    def execute_command(self, *args, **options):
        pool = self.connection_pool
        cmd = args[0]
        conns = []
        ts = time.time()
        backend = 'None'
        try:
            for i in xrange(0, self.trycount):
                conn = pool.get_connection(cmd, **options)
                backend = '%s_%d' % (conn.host, conn.port)
                conns.append(conn)
                try:
                    conn.send_command(*args)
                    r = self.parse_response(conn, cmd, **options)
                    sp = time.time() - ts
                    self.metrics.add_call_metrics(cmd, sp, True, backend=backend)
                    if self.slowlogging > 0 and sp > self.slowlogging:
                        logging.info('%s %s %s elapsed:%dus' %
                                (self.cluster, backend, str(args)[:128], int(sp * 1000000)))
                    return r
                except (ConnectionError, TimeoutError) as e:
                    conn.disconnect()
                    pool.mark_dead(conn)
                    if i + 1 == self.trycount:
                        raise
        except:
            #logging.info('execute_command fail')
            self.metrics.add_call_metrics(cmd, time.time() - ts, False, backend=backend)
            raise
        finally:
            for c in conns:
                pool.release(c)

    def pipeline(self, transaction=False, shard_hint=None, retry=1, name='pipeline', slowlogging=None):
        if transaction:
            return None
        if slowlogging == None:
            slowlogging = self.slowlogging
        return BatchPipeline(self, retry, name, slowlogging, transaction, shard_hint)

    def mset(self, *args, **kwargs):
        if args:
            if len(args) != 1 or not isinstance(args[0], dict):
                raise RedisError('MSET requires **kwargs or a single dict arg')
            kwargs.update(args[0])
        p = self.pipeline(retry=self.retry, name='mset', slowlogging=self.slowlogging)
        for k, v in kwargs.iteritems():
            p.set(k, v)
        return p.execute()


class BatchPipeline(StrictPipeline):
    def __init__(self, client, retry=1, name='pipeline', slowlogging=0, transaction=False, shard_hint=None):
        StrictPipeline.__init__(self, client.connection_pool, client.response_callbacks, transaction, shard_hint)
        self.client = client
        self.name = name
        if not self.name or len(self.name) == 0:
            self.name == 'pipeline'
        self.metrics = self.client.metrics
        self.trycount = 1
        self.retry = retry
        if retry > 0:
            self.trycount += retry
        self.slowlogging = slowlogging
        if slowlogging == None:
            self.slowlogging = client.slowlogging
    
    def execute(self, raise_on_error=True):
        stack = self.command_stack
        if not stack:
            return []

        ts = time.time()
        conns = []
        backend = 'None'
        pool = self.connection_pool
        c = None
        try:
            for i in xrange(0, self.trycount):
                try:
                    response = []
                    c = pool.get_connection('MULTI', self.shard_hint)
                    conns.append(c)
                    backend = '%s_%d' % (c.host, c.port)
                    cmds = chain.from_iterable(starmap(c.pack_command, [args for args, options in stack]))
                    c.send_packed_command(cmds)
                    for args, options in stack:
                        st = True
                        try:
                            r = self.parse_response(c, args[0], **options)
                            response.append(r)
                        except ResponseError:
                            response.append(sys.exc_info()[1])
                            st = False
                        self.metrics.add_call_metrics(args[0], -1, st, backend=backend)

                    if raise_on_error:
                        self.raise_first_error(stack, response)

                    sp = time.time() - ts
                    self.metrics.add_call_metrics(self.name, sp, True, backend)
                    if self.slowlogging > 0 and sp > self.slowlogging:
                        logging.info('%s %s %s cmdcnt:%d elapsed:%dus' %
                                (self.client.cluster, backend, self.name, len(stack), int(sp * 1000000)))
                    return response
                except (ConnectionError, TimeoutError) as e:
                    c.disconnect()
                    pool.mark_dead(c)
                    if i + 1 == self.trycount:
                        raise
        except:
            self.metrics.add_call_metrics(self.name, time.time() - ts, False, backend)
            raise
        finally:
            self.reset()
            for c in conns:
                pool.release(c)


# coding: utf-8

import json
import logging
import random
import re
import socket
import sys
import time
import requests
import csv

from pyutil.consul.bridge import translate_one

if sys.version_info[0] == 2:
    PY3 = False
    from urllib import urlencode
    from httplib import HTTPConnection

elif sys.version_info[0] == 3:
    PY3 = True
    from urllib.parse import urlencode
    from http.client import HTTPConnection

logging.getLogger("urllib").setLevel(logging.WARNING)
logging.basicConfig(
    stream=sys.stderr,
    format='[%(asctime)s] - [%(levelname)s] - %(filename)s - %(funcName)s - +%(lineno)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger("tqs")
logger.setLevel(logging.INFO)

DEFAULT_TIMEOUT = 10

DEFAULT_CLUSTER = "default"
TEST_CLUSTER = "test"
VA_AWS_CLUSTER = "va_aws"
VA_TEST_CLUSTER = "va_test"
CN_PRIEST_CLUSTER = "cn_priest"
VA_PRIEST_CLUSTER = "va_priest"

TQS_API_SERVER_PSM_TEST = "data.olap.tqs_test"
TQS_API_SERVER_PSM_CN = "data.olap.tqs"
TQS_API_SERVER_PSM_AWS = "data.olap.tqs_va_aws"
TQS_API_SERVER_PSM_TEST_VA = "data.olap.tqs_va_test"
TQS_API_SERVER_PSM_CN_PRIEST = "data.olap.tqs_cn_priest"
TQS_API_SERVER_PSM_VA_PRIEST = "data.olap.tqs_va_priest"

TQS_APPID_HEADER = "X-TQS-AppID"
TQS_APPKEY_HEADER = "X-TQS-AppKey"
FINAL_STATUS = {'AnalysisFailed', 'Completed', 'Cancelled', 'Failed'}
SUCCESS_STATUS = {'Completed'}
CANCEL_STATUS = {'Cancelled'}
FAILURE_STATUS = {'AnalysisFailed', 'Cancelled', 'Failed'}


class TQSException(Exception):
    def __init__(self, code, msg):
        self.code = code
        self.msg = msg
        Exception.__init__(self, "%s:%s" % (self.code, self.msg))


class TQSClient(object):

    def __init__(self, appId, appKey, **kwargs):
        self.appId = appId
        self.appKey = appKey
        self.timeout = kwargs.get("timeout", DEFAULT_TIMEOUT)
        self.cluster = kwargs.get("cluster", DEFAULT_CLUSTER)
        if self.cluster == TEST_CLUSTER:
            self.worker_psm = "data.olap.tqs_worker_test"
        elif self.cluster == DEFAULT_CLUSTER:
            self.worker_psm = "data.olap.tqs_worker"
        elif self.cluster == VA_AWS_CLUSTER:
            self.worker_psm = "data.olap.tqs_worker_va_aws"
        elif self.cluster == VA_TEST_CLUSTER:
            self.worker_psm = "data.olap.tqs_worker_va_test"
        elif self.cluster == CN_PRIEST_CLUSTER:
            self.worker_psm = "data.olap.tqs_worker_cn_priest"
        elif self.cluster == VA_PRIEST_CLUSTER:
            self.worker_psm = "data.olap.tqs_worker_va_priest"
        else:
            self.worker_psm = "data.olap.tqs_worker"

    def _get_conn(self):
        if self.cluster == TEST_CLUSTER:
            host_port = random.choice(translate_one(TQS_API_SERVER_PSM_TEST))
        elif self.cluster == DEFAULT_CLUSTER:
            host_port = random.choice(translate_one(TQS_API_SERVER_PSM_CN))
        elif self.cluster == VA_AWS_CLUSTER:
            host_port = random.choice(translate_one(TQS_API_SERVER_PSM_AWS))
        elif self.cluster == VA_TEST_CLUSTER:
            host_port = random.choice(translate_one(TQS_API_SERVER_PSM_TEST_VA))
        elif self.cluster == CN_PRIEST_CLUSTER:
            host_port = random.choice(translate_one(TQS_API_SERVER_PSM_CN_PRIEST))
        elif self.cluster == VA_PRIEST_CLUSTER:
            host_port = random.choice(translate_one(TQS_API_SERVER_PSM_VA_PRIEST))
        return HTTPConnection(host=host_port[0], port=host_port[1], timeout=self.timeout)

    def _req(self, method, url, body=None, headers=None):
        _headers = {'Content-Type': 'application/json',
                    TQS_APPID_HEADER: self.appId,
                    TQS_APPKEY_HEADER: self.appKey
                    }
        if headers:
            _headers.update(headers)

        _body = None
        if body:
            _body = json.dumps(body)

        conn = self._get_conn()
        conn.request(method, url, _body, _headers)
        response = conn.getresponse()
        status = response.status
        data = response.read()

        if status >= 400:
            if data:
                error = json.loads(data)
                raise TQSException(error['code'], error['message'])
            else:
                raise TQSException(-1, "HTTP request failed with status: %s" % status)
        return status, data

    def _create_query(self, username, sql, dryRun=False, conf=None, skipCostAnalysis=False, name=None, priority=None):
        '''
        create query, and return jobId
        '''
        if conf:
            assert isinstance(conf, dict), type(conf)

        url = "/api/v1/queries"
        if conf:
            lookup = {'user': username, 'query': sql, 'dryRun': dryRun, 'conf': json.dumps(conf),
                      'skipCostAnalysis': skipCostAnalysis}
            if name:
                lookup['name'] = name
            if priority:
                lookup['priority'] = priority
        else:
            lookup = {'user': username, 'query': sql, 'dryRun': dryRun, 'skipCostAnalysis': skipCostAnalysis}
            if name:
                lookup['name'] = name
            if priority:
                lookup['priority'] = priority
        status, data = self._req('POST', url, lookup)
        result = json.loads(data)['jobId']
        return result

    def _sync_execute_query(self, username, sql, conf=None, skipCostAnalysis=False, name=None, priority=None):
        jobId = self._create_query(username, sql, dryRun=False, conf=conf,
                                   skipCostAnalysis=skipCostAnalysis, name=name, priority=priority)
        self._wait_to_finish(jobId)
        return self.get_job(jobId)

    def _async_execute_query(self, username, sql, conf=None, skipCostAnalysis=False, name=None, priority=None):
        '''
        retrun job
        '''
        jobId = self._create_query(username, sql, dryRun=False, conf=conf,
                                   skipCostAnalysis=skipCostAnalysis, name=name, priority=priority)
        return self.get_job(jobId)

    def _parse_hive_command(self, query_segment):
        hive_cmd_dict = dict()
        convert_query = query_segment.strip()

        # set command match
        set_match = re.compile(r'\s*[sS][eE][tT]\s+(.*)=\s*(.*)\s*').match(convert_query)
        if set_match:
            hive_cmd_dict[set_match.group(1).strip()] = set_match.group(2).strip()

        # add command match
        add_match = re.compile(r'\s*[aA][dD][dD]\s+(.*)\s*').match(convert_query)
        if add_match:
            hive_cmd_dict["%s %s" % ('add', add_match.group(1).strip())] = ""
        return hive_cmd_dict

    def _wait_to_finish(self, job_id):
        loop_start = time.time()
        ori_worker = ''
        ori_engine = ''
        ori_tracking_url = ''
        while True:
            try:
                job = self.get_job(job_id)
                engine_type = job.get('engineType')
                progress = job.get('progress')
                stage_info = list()
                if progress:
                    progress_dict = json.loads(progress)
                    stages = progress_dict.get('stages')
                    if stages:
                        if engine_type == 'Hive':
                            for stage, mr_info in stages.items():
                                stage_info.append('stage%s:map-%d%%,reduce-%d%%' % (stage, mr_info[0], mr_info[1]))
                        elif engine_type == 'Spark':
                            for stage, mr_info in stages.items():
                                stage_info.append('%d%%' % (mr_info[0]))

                    # print hive task tracking url
                    tracking_url = progress_dict.get('trackingUrl')
                    if tracking_url and ori_tracking_url != tracking_url:
                        ori_tracking_url = tracking_url
                        logger.info("application tracking url-%s", ori_tracking_url)

                    # print query engine when switch
                    engine = progress_dict.get('executeEngine')
                    if engine and ori_engine != engine:
                        ori_engine = engine
                        logger.info("tqs_task_id: %s executed on query engine-%s" % (job_id, ori_engine))

                    # print worker address when switch
                    worker = progress_dict.get('workerAddress')
                    if worker and worker != ori_worker:
                        ori_worker = worker
                        logger.info("tqs_task_id: %s executed on PSM-%s worker-%s" % (
                            job_id, self.worker_psm, ori_worker))

                    # print progress information
                    logger.info("tqs_task_id: %s, engine: %s, status: %s, progress : %s" % (
                        job_id, job['engineType'], job['status'], ','.join(stage_info)))
                else:
                    logger.info("tqs_task_id: %s, engine: %s, status: %s" % (job_id, job['engineType'], job['status']))
                if job['status'] in FINAL_STATUS:
                    break
            except socket.timeout as err:
                logger.error(err)
            except socket.error as err:
                logger.error(err)
            interval = self._get_sleep_interval(loop_start)
            time.sleep(interval)

    def _get_sleep_interval(self, start_time):
        elapsed = time.time() - start_time
        if elapsed < 60.0:
            return 2
        return 5

    def get_job(self, job_id):
        '''
        get job by jobId

        {
            "appId": "xxxxxxx",
            "endTime": "2017-10-24 16:55:06.0",
            "engineType": "Hive",
            "id": 67,
            "logUrl": "http://tqs.byted.org/download/results/20171024/16/67_yYDD5W7sGyXYLx84.log",
            "maxResults": 50,
            "query": "select * from user_action_log_daily where date=20170301 limit 10",
            "resultUrl": "http://tqs.byted.org/download/results/20171024/16/67_9zlwRi5dT4fSdy6U.csv",
            "startTime": "2017-10-24 16:55:00.0",
            "status": "Completed",
            "userName": "yangchaozhong",
            "progress": "{"trackingUrl":"http://10.10.163.167:9003/api/v1/applications","stages":{"stage-143,140,144,141,145,142":[2,0]}}"
        }
        '''
        url = "/api/v1/jobs/%s" % job_id
        status, data = self._req('GET', url)
        result = json.loads(data)
        return result

    def execute_query(self, username, sql, async=False, conf=None, skipCostAnalysis=False, name=None, priority=None):
        '''
        如果你申请的 Application 是`普通应用`,这里传入的 username 必须是 Application owner 的名字（公司邮箱前缀）。
        相反，如果你申请的 Application 是`超级应用`，此处无限制，可以是公司任意一个已存在的在职员工用户名（注意：权力越大，责任越大，请确保你的应用有做用户身份认证）。
        '''
        if async:
            # return job
            return self._async_execute_query(username, sql, conf=conf, skipCostAnalysis=skipCostAnalysis, name=name, priority=priority)
        else:
            # return job
            return self._sync_execute_query(username, sql, conf=conf, skipCostAnalysis=skipCostAnalysis, name=name, priority=priority)

    def execute_multiple_query(self, username, sql, async=False, conf=None, skipCostAnalysis=False, name=None, priority=None):
        job_conf = dict()
        if not conf:
            job_conf.update(conf)
        execute_query = list()
        for query_segment in sql.strip().strip(';').split(';'):
            query_segment_dict = self._parse_hive_command(query_segment)
            if bool(query_segment_dict):
                job_conf.update(query_segment_dict)
            else:
                execute_query.append(query_segment)
        query_sql = ';'.join(execute_query)
        if async:
            return self._async_execute_query(username, query_sql, conf=job_conf, skipCostAnalysis=skipCostAnalysis)
        else:
            return self._sync_execute_query(username, query_sql, conf=job_conf, skipCostAnalysis=skipCostAnalysis)

    def cancel_query(self, job_id):
        url = "/api/v1/jobs/%s/cancel" % job_id
        self._req('POST', url)

    def explain_query(self, username, sql, conf=None):
        job_id = self._create_query(username, sql, dryRun=True, conf=conf)
        self._wait_to_finish(job_id)
        job = self.get_job(job_id)
        return {'status': job['status']}

    def fetch_results(self, job_id, sample_result=True):
        '''
        {'rows': [['col'], [1], [2], [3]], 'url': 'http://tqs.byted.org/download/results/20171024/16/67_9zlwRi5dT4fSdy6U.csv'}
        '''
        url = "/api/v1/queries/%s" % job_id
        status, data = self._req('GET', url)
        if data:
            try:
                result = json.loads(data)
                if not sample_result:
                    with requests.Session() as s:
                        download = s.get(result['url'])
                        decoded_content = download.content.decode('utf-8')
                        cr = csv.reader(decoded_content.splitlines(), delimiter=',')
                        return list(cr)
                else:
                    return result
            except:
                return None
        else:
            return None

    def download_results(self, job_id, call_back):
        url = "/api/v1/queries/%s" % job_id
        status, data = self._req('GET', url)
        if data:
            result = json.loads(data)
            with requests.Session() as s:
                download = s.get(result['url'], stream=True)
                for line in download.iter_lines():
                    if line:
                        decoded_line = line.decode('utf-8')
                        call_back(decoded_line)

    def fetch_results_status(self, job_id):
        '''
        {'url': 'http://tqs.byted.org/download/results/20171024/16/67_9zlwRi5dT4fSdy6U.csv', 'fileSize' : 787283}
        '''
        url = "/api/v1/queries/%s/status" % job_id
        status, data = self._req('GET', url)
        if data:
            try:
                result = json.loads(data)
                return result
            except:
                return None
        else:
            return None

    def fetch_failure_log(self, job_id):
        '''
        {'rows': 'failure log information', 'url' : 'http://tqs.byted.org/download/results/20171024/16/67_9zlwRi5dT4fSdy6U.failure.log'}
        '''
        url = "/api/v1/queries/%s/failure" % job_id
        status, data = self._req('GET', url)
        if data:
            try:
                result = json.loads(data)
                return result
            except:
                return None
        else:
            return None

    def fetch_query_log(self, job_id):
        '''
        {'rows': 'query log information', 'url' : 'http://tqs.byted.org/download/results/20171024/16/67_9zlwRi5dT4fSdy6U.failure.log'}
        '''
        url = "/api/v1/queries/%s/log" % job_id
        status, data = self._req('GET', url)
        if data:
            try:
                result = json.loads(data)
                return result
            except:
                return None
        else:
            return None

    def is_finished(self, job_id):
        job = self.get_job(job_id)
        return job['status'] in FINAL_STATUS

    def is_success(self, job_id):
        job = self.get_job(job_id)
        return job['status'] in SUCCESS_STATUS

    def is_failure(self, job_id):
        job = self.get_job(job_id)
        return job['status'] in FAILURE_STATUS

    def is_cancel(self, job_id):
        job = self.get_job(job_id)
        return job['status'] in CANCEL_STATUS


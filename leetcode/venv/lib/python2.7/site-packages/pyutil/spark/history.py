from snakebite.errors import FileNotFoundException
from pyutil.hadoop.hdfs import get_hdfs_client
import time
import json
import os


class HistorySession:
    def __init__(self):
        self.hdfs_client = get_hdfs_client()
        self.spark_log_dir = "/user/tiger/spark_history"
        self.spark_log_dir_bak = "/user/tiger/spark_history_bak"

    def get_exceptions(self, app_id):
        """
        Get the Exceptions for a given app_id in a list,
        every item in the list is dict,
        the key is the job_id, and the value is the exception
        """
        exceptions = []
        local_file_name = str(app_id) + "-tmp"
        path = self.spark_log_dir + "/" + str(app_id)
        try:
            ans = self.hdfs_client.copyToLocal([path], local_file_name)
            ans.next()
            with open(local_file_name, "r") as tmp_fin:
                for line in tmp_fin:
                    if "SparkListenerJobEnd" in line:
                        info = json.loads(line)
                        job_result = info["Job Result"]
                        if job_result["Result"] == "JobFailed":
                            job_id = info["Job ID"]
                            exception = job_result["Exception"]["Message"]
                            exceptions.append({job_id: exception})
        except FileNotFoundException as e:
            exceptions.append({"-1": "The log file is no exist yet"})
        except Exception as e:
            exceptions.append({"-1": e})
        finally:
            if os.path.exists(local_file_name):
                os.remove(local_file_name)

        return exceptions

    def close(self):
        pass

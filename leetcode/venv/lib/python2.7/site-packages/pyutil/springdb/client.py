# -*- coding: utf-8 -*-

import time, logging, functools, threading
import redis, os

from pyutil.program.functool import retries
import pyutil.program.metrics2 as metrics
import pyutil.twemproxy as twemproxy

from prefix import *

class ClientBase(object):
    '''
    Construction is not cost-free, you should avoid constructing it too
    frequently. Moreover, it's thread-safe.
    '''

    SUPPORTED_COMMANDS = []

    ABASE_COMMANDS = []

    # static data
    _metrics_defined = False
    _shared_pools = {}
    _shared_pools_lock = threading.Lock()

    perf_test = os.getenv('TCE_PERF_TEST', None)
    perf_test_prefix = os.getenv('TCE_PERF_PREFIX', 'tce_perf_test_a3b30390ca0c_')
    perf_test_whitelist = os.getenv('TCE_PERF_WHITELIST', '')

    def __init__(self, cluster, table, socket_timeout=0.25, socket_connect_timeout=0.1, module='springdb',
                 zone='auto', thread_local_pool=False, find_connection_timeout=0.1, auto_conf_pool=False,
                 conf_interval_time=60,
                 max_retries=0, retry_delay=1, retry_backoff=2,
                 caller=None):
        '''
        max_retries - 最大重试次数, None: 无限次, >=0: 指定次数
        retry_delay - retry的间隔 (in seconds)
        retry_backoff - 重试时间随次数增加, 每次重试的delay乘以此值
        '''
        ts = time.time()
        if isinstance(cluster, basestring):
            conf_path = twemproxy.ClientConf.get_conf_path(module, zone)
            conf = twemproxy.ClientConf.get_cluster_conf(module, cluster, zone)
            if conf is None:
                raise ValueError("cluster '%s' not found in conf '%s'"
                                 % (cluster, conf_path))
            if table not in conf['tables']:
                raise ValueError("table '%s' not found for cluster '%s' in conf"
                                 "'%s'" % (table, cluster, conf_path))
            self.cluster = cluster
            self.real_cluster = conf['real_cluster']
            self.servers = conf['servers']
            self.table = table
        elif isinstance(cluster, list):
            # a list is allowed only for debug or test, do not use this online
            # i.e. ['127.0.0.1:5555', '127.0.0.1:6666']
            self.cluster = 'default'
            self.real_cluster = 'default'
            self.servers = cluster
            self.table = table
        else:
            raise TypeError("argument cluster '%r' should be list or string"
                            % cluster)
        self.caller = caller or 'unknown'
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.retry_backoff = retry_backoff
        self._define_metrics(self.cluster, self.real_cluster, self.table)

        (self.pool, self.pool_is_shared) = ClientBase._get_pool(
                self.servers, socket_timeout=socket_timeout,
                socket_connect_timeout=socket_connect_timeout,
                find_connection_timeout=find_connection_timeout,
                thread_local_pool=thread_local_pool, auto_conf_pool=auto_conf_pool,
                conf_interval_time=conf_interval_time)
        self.handle = PrefixedRedis(connection_pool=self.pool)

        prefix = '[%s]' % self.table
        # whitelist format springdb:tablename1&springdb:tablename2
        if self.perf_test == '1':
            whitelist = self.perf_test_whitelist.split('&')
            iswhite_table = False
            for item in whitelist:
                if item == ('springdb:%s' % self.table):
                   iswhite_table = True
                   break
            if not iswhite_table:
                prefix = '[%s]_%s' % (self.table, self.perf_test_prefix)

        self.handle.set_prefix(prefix)
        self.handle.set_supported_commands(self.SUPPORTED_COMMANDS + self.ABASE_COMMANDS)
        if self.max_retries:
            self._execute = retries(self.max_retries + 1, delay=self.retry_delay, backoff=self.retry_backoff)(self._execute)

        self.tags = {'cluster': self.cluster, 'real_cluster': self.real_cluster,
                'table': self.table, 'lang': 'py', 'caller': self.caller}

        metrics.emit_counter('init.count', 1, self.METRIC_PREFIX, self.tags)
        metrics.emit_timer('init.latency', (time.time() - ts) * 1000000,
                           self.METRIC_PREFIX, self.tags)

        logging.debug('%s init. cluster=%r real_cluster=%s '
                      'servers=%r table=%s pool_is_shared=%d socket_timeout=%s '
                      'socket_connect_timeout=%s'
                      % (self.CLIENT_NAME, self.cluster, self.real_cluster,
                      self.servers, self.table, self.pool_is_shared,
                      socket_timeout, socket_connect_timeout))

    def _scanrow(self, row, limit = 0, offset = 0, target = ""):
        table_prefix = "[{}]".format(self.table)
        if target != "":
            target = table_prefix + target
        pieces = [row, str(limit), str(offset), target]
        return self.handle.execute_command('SCANROW', *pieces)

    def _scanrow_iter(self, row, batch_size=1024, target="", reverse=False):
        if batch_size <= 0:
            raise ValueError("batch size must be > 0")
        first_round = True
        batch_size *= -1 if reverse else 1
        while True:
            offset = 0 if first_round else 1
            offset *= -1 if reverse else 1
            first_round = False
            kvs = self._scanrow(row, batch_size, offset, target)
            for kv in kvs:
                yield kv[0], kv[1]
            if len(kvs) < abs(batch_size):
                raise StopIteration()
            else:
                target = kvs[-1][0]

    def _execute(self, cmd, *args, **kwargs):
        ts = time.time()
        tags = self.tags
        tags['cmd'] = cmd

        try:
            if cmd in self.ABASE_COMMANDS:
                func = getattr(self, '_' + cmd)
            else:
                func = getattr(self.handle, cmd)
            result = func(*args, **kwargs)
            return result
        except Exception, ex: # TODO extra processing for connection error?
            logging.warning('exception in %s execute: %s'
                            % (self.CLIENT_NAME, str(ex)))
            metrics.emit_counter('fail', 1, self.METRIC_PREFIX, tags)
            raise
        finally:
            metrics.emit_counter('count', 1, self.METRIC_PREFIX, tags)
            metrics.emit_timer('latency', (time.time() - ts) * 1000000,
                               self.METRIC_PREFIX, tags)

    def __getattr__(self, cmd):
        if cmd in self.__dict__:
            return self.__dict__[cmd]
        elif cmd in self.__class__.__dict__:
            return self.__class__.__dict__[cmd]

        if cmd in self.SUPPORTED_COMMANDS or cmd in self.ABASE_COMMANDS:
            ret = self.__dict__[cmd] = functools.partial(self._execute, cmd)
            return ret
        else:
            raise NotImplementedError("cmd '%s' not supported" % cmd)

    def pipeline(self):
        p = self.handle.pipeline(transaction=False)
        p.set_stats_callback(self.emit_pipeline_metrics)
        return p

    def emit_pipeline_metrics(self, latency, batch_num, ok, command_stack):
        metrics.emit_timer('pipeline.latency', latency * 1000000,
                           self.METRIC_PREFIX, self.tags)
        if batch_num > 0:
            metrics.emit_timer('pipeline.a_latency', latency / batch_num * 1000000,
                               self.METRIC_PREFIX, self.tags)
            metrics.emit_timer('pipeline.batch_num', batch_num,
                               self.METRIC_PREFIX, self.tags)

        if not ok:
            tags = self.tags
            # we don't know the exact cmd of error, use pipeline instead
            tags['cmd'] = 'pipeline'
            metrics.emit_counter('fail', 1, self.METRIC_PREFIX, tags)
        else:
            try:
                # command_stack is internal in redis.Redis, whose implemention
                # may be changed, so must use try/except
                merged = {}
                for (args, options) in command_stack:
                    cmd = args[0].lower()
                    if cmd not in merged:
                        merged[cmd] = 1
                    else:
                        merged[cmd] += 1
                tags = self.tags
                for cmd, count in merged.items():
                    tags['cmd'] = cmd
                    metrics.emit_counter('count', count,
                                         self.METRIC_PREFIX, tags)
            except Exception, ex:
                logging.warning('Exception. command_stack metrics: ' + str(ex))

    def _define_metrics(self, cluster, real_cluster, table):
        metrics.define_tagkv('cluster', [cluster])
        metrics.define_tagkv('real_cluster', [real_cluster])
        metrics.define_tagkv('table', [table])
        metrics.define_tagkv('caller', [self.caller])
        metrics.define_tagkv('lang', ['py'])

        metrics.define_counter('init.count', prefix=self.METRIC_PREFIX)
        metrics.define_timer('init.latency', prefix=self.METRIC_PREFIX)

        metrics.define_counter('count', prefix=self.METRIC_PREFIX)
        metrics.define_counter('fail', prefix=self.METRIC_PREFIX)
        metrics.define_timer('latency', prefix=self.METRIC_PREFIX)

        # TODO add connect.count, connect.fail ?
        # TODO add bytes metrics
        #metrics.define_counter('bytes_written', prefix=self.METRIC_PREFIX)
        #metrics.define_counter('bytes_read', prefix=self.METRIC_PREFIX)

        metrics.define_timer('pipeline.latency', prefix=self.METRIC_PREFIX)
        metrics.define_timer('pipeline.a_latency', prefix=self.METRIC_PREFIX)
        metrics.define_timer('pipeline.batch_num', prefix=self.METRIC_PREFIX)

        metrics.define_tagkv('cmd', self.SUPPORTED_COMMANDS)
        metrics.define_tagkv('cmd', self.ABASE_COMMANDS)
        metrics.define_tagkv('cmd', ['pipeline'])

    @staticmethod
    def _get_pool(servers, thread_local_pool=False, auto_conf_pool=False, cluster=None,
                  find_connection_timeout=0.1, conf_interval_time=60, **connection_kwargs):
        '''
        @return: pool, is_shared
        '''
        sign = unicode((servers, connection_kwargs, ))
        with ClientBase._shared_pools_lock:
            if sign in ClientBase._shared_pools:
                pool = ClientBase._shared_pools[sign]
                return pool, True
            else:
                if thread_local_pool:
                    pool = twemproxy.ThreadLocalConnectionPool(
                            servers, find_connection_timeout=find_connection_timeout,
                            **connection_kwargs)
                elif auto_conf_pool and cluster:
                    pool = twemproxy.AutoConfRedisConnectionPool(cluster_name=cluster,
                                                                 conf_interval_time=conf_interval_time,
                                                                 conf_file="/opt/tiger/ss_conf/ss/springdb.conf",
                                                                 **connection_kwargs)
                else:
                    pool = twemproxy.RandomRedisConnectionPool(
                        servers, **connection_kwargs)
                ClientBase._shared_pools[sign] = pool
                return pool, False

class SpringDBClient(ClientBase):
    '''
    Construction is not cost-free, you should avoid constructing it too
    frequently. Moreover, it's thread-safe.
    '''

    # constants
    CLIENT_NAME = 'SpringDBClient'
    METRIC_PREFIX = 'inf.springdbclient' # metric prefix
    SUPPORTED_COMMANDS = [
        'del', 'delete', 'expire',

        'set', 'setnx', 'setex', 'getset', 'incr', 'decr', 'incrby', 'decrby',
        'append',
        # 'mset', not supported in twemproxy
        # 'mget', TODO harmful for twemproxy, use pipeline instead
        'get', 'mget', 'exists',

        'hset', 'hmset', 'hdel', 'hincrby', 'hincrbyfloat',
        'hget', 'hmget', 'hexists', 'hlen', 'hgetall', 'hkeys', 'hvals',

        'zadd', 'zincrby', 'zrem', 'zremrangebyrank', 'zremrangebyscore',
        'zcard', 'zcount', 'zscore', 'zrank', 'zrevrank',
        'zrange', 'zrangebyscore', 'zrevrange', 'zrevrangebyscore',

        'lpush', 'rpush', 'lpop', 'rpop',
        'llen', 'lindex', 'lrange', 'ttl',
        'eval', 'evalsha',

        # for cas
        'xget', 'xset',
    ]

    ABASE_COMMANDS = ['scanrow', 'scanrow_iter']

    # static data
    _zone = 'auto'

    @staticmethod
    def set_zone(zone):
        '''
        use /etc/ss_conf as default conf directory

        @param zone: auto/online/offline/test
        '''
        SpringDBClient._zone = zone

    def __init__(self, *args, **kwargs):
        '''
        @param string cluster, the cluster name
        @param string table, the table name which will add as prefix in the key
        @param socket_timeout in seconds
        @param socket_connect_timeout in seconds

        example:
            db = SpringDBClient('springdb_sandbox', 'sandbox', 0.25, 0.1)

        Please do not pass in a server list as cluster, use the cluster name
        instead.
        '''
        kwargs['module'] = 'springdb'
        kwargs['zone'] = SpringDBClient._zone
        ClientBase.__init__(self, *args, **kwargs)

if __name__ == '__main__':
    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s')
    logging.getLogger().setLevel(logging.DEBUG)

    #c = SpringDBClient(['10.4.17.163:5390'], 'unittest')
    #c = SpringDBClient('springdb_sandbox', 'sandbox')
    SpringDBClient.set_zone('online')
    c = SpringDBClient('springdb_impression', 'short_packed')
    key = '12:1592632028:16254'
    print key
    val = c.get(key)
    print 'val len', len(val)



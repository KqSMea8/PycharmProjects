#!/usr/bin/env python
# -*- coding: utf-8 -*-

import argparse, logging, time, sys, math, random, copy, multiprocessing
from pyutil.springdb.client import SpringDBClient

import re, string
def convert_to_printable(s):
    expression = "[^%s%s%s ]+" % (string.digits, string.letters, string.punctuation)
    return re.sub(expression, ".", s)

import hashlib
def hash_val(data, salt=''):
    return hashlib.md5(str(data)+str(salt)).hexdigest()

import signal
def sig_handler(signum, frame):
    print 'process %s caught sig %d' % (multiprocessing.current_process().name, signum)
    for p in multiprocessing.active_children():
        p.terminate()
    sys.exit(-1)

CASE_LIST = [
    'set', 'get', 'getthenset', 'mget', #'pipeline_mget',
    'zadd', 'zrangebyscore',
    'hset', 'hmset', 'hget', 'hmget',
    'rpush', 'lrange',
]
WRITE_CMD_LIST = set([
    'set',
    'zadd',
    'hset', 'hmset',
    'rpush',
])

class Checker(object):
    def __init__(self, args):
        self.args = args

    def run(self):
        worker = Worker(self.args)

        for case_name in self.args.case_list:
            logging.info('start case %s' % case_name)

            ts = time.time()
            for loop in range(self.args.loop):
                worker.run(case_name)
            te = time.time()

            sys.stdout.write('\n')
            sys.stdout.flush()
            worker.stats.print_to_log(te - ts, False)

            time.sleep(self.args.sleep)

class Perf(object):

    def __init__(self, args):
        self.args = args

    def run(self):
        ''' main process '''
        signal.signal(signal.SIGALRM, sig_handler)
        signal.signal(signal.SIGINT, sig_handler)

        stats_collector = StatsCollector(self.args)
        worker = Worker(self.args)
        worker.gen_cached_value()

        for loop in range(self.args.loop):
            for case_name in self.args.case_list:
                stats_queue = multiprocessing.JoinableQueue()
                stats_process = multiprocessing.Process(target=stats_collector.run, args=(stats_queue,))
                stats_process.start()
                #logging.debug('pid: stats_process %d' % stats_process.pid)

                logging.info('start case %s' % case_name)
                jobs = []
                for idx in range(self.args.concurrency):
                    p = multiprocessing.Process(target=worker.run, args=(case_name, idx, stats_queue))
                    jobs.append(p)
                    p.start()
                    #logging.debug('pid: job %d' % p.pid)
                for j in jobs:
                    j.join()

                stats_queue.join() # wait all item in queue is processed
                stats_queue.put('END')
                stats_process.join()

            time.sleep(self.args.sleep)

        logging.info('main process exited')

class Stats(object):
    def __init__(self, name, case_list):
        self.name = name
        self.case_list = case_list
        self.reset_all()

    def gen_empty_stats(self):
        empty_stat = {'count': 0, 'total_seconds': 0, 'succeed': 0, 'hits': 0, 'misses': 0}
        result = {c: copy.deepcopy(empty_stat) for c in self.case_list}
        result['name'] = self.name
        return result

    def reset(self):
        self.current = self.gen_empty_stats()
        self.timestamp = time.time()
        self.count = 0
        self.collected_from = set()

    def reset_all(self):
        self.reset()

        self.accumulate = self.gen_empty_stats()
        self.accumulate_start = time.time()


    def throughput_control(self, throughput):
        if throughput <= 0:
            return
        elapsed = time.time() - self.timestamp
        expected = float(self.count) / throughput
        if expected > elapsed:
            time.sleep(expected - elapsed)

    def get(self):
        return self.current

    def add(self, case, seconds, succeed=True):
        self.count += 1
        self.current[case]['count'] += 1
        self.current[case]['total_seconds'] += seconds
        self.current[case]['succeed'] += 1 if succeed else 0
        self.accumulate[case]['count'] += 1
        self.accumulate[case]['total_seconds'] += seconds
        self.accumulate[case]['succeed'] += 1 if succeed else 0

    def add_hits_misses(self, case, hits, misses):
        self.current[case]['hits'] += hits
        self.current[case]['misses'] += misses
        self.accumulate[case]['hits'] += hits
        self.accumulate[case]['misses'] += misses

    def collect(self, stats):
        self.collected_from.add(stats['name'])
        for c, stat in stats.items():
            if c == 'name':
                continue
            self.count += stat['count']
            self.current[c]['count'] += stat['count']
            self.current[c]['total_seconds'] += stat['total_seconds']
            self.current[c]['succeed'] += stat['succeed']
            self.current[c]['hits'] += stat['hits']
            self.current[c]['misses'] += stat['misses']
            self.accumulate[c]['count'] += stat['count']
            self.accumulate[c]['total_seconds'] += stat['total_seconds']
            self.accumulate[c]['succeed'] += stat['succeed']
            self.accumulate[c]['hits'] += stat['hits']
            self.accumulate[c]['misses'] += stat['misses']

    def print_to_log(self, stats_interval=1, is_perf_test=True):
        for c in self.case_list:
            if self.current[c]['count'] == 0:
                continue

            cur = self.current[c]
            acc = self.accumulate[c]

            cur_avg_time = cur['total_seconds'] / cur['count'] if cur['count'] > 0 else 0
            cur_succeed_rate = cur['succeed'] / cur['count'] if cur['count'] > 0 else 1
            cur_qps = cur['count'] / stats_interval

            acc_avg_time = acc['total_seconds'] / acc['count'] if acc['count'] > 0 else 0
            acc_qps = acc['count'] / (time.time() - self.accumulate_start)

            hits_misses_msg = ''
            if cur['hits'] > 0 or cur['misses'] > 0:
                total = cur['hits'] + cur['misses']
                hits_misses_msg = ' hit_rate=%.2f' % (float(cur['hits']) / total)

            if is_perf_test:
                msg = 'case=%s concurrency=%d cur: qps=%d avg(ms)=%.2f succeed=%d succeed_rate=%.2f%s' \
                    ' total: qps=%d avg(ms)=%.2f count=%d' \
                    % (c, len(self.collected_from), cur_qps, cur_avg_time * 1000, cur['succeed'],
                    cur_succeed_rate, hits_misses_msg,
                    acc_qps, acc_avg_time * 1000, acc['count'])
            else:
                msg = 'case=%s count=%d qps=%d avg(ms)=%.2f succeed=%d succeed_rate=%.2f%s' \
                    % (c, cur['count'], cur_qps, cur_avg_time * 1000, cur['succeed'],
                    cur_succeed_rate, hits_misses_msg)
            logging.info(msg)

class StatsCollector(object):

    def __init__(self, args):
        self.args = args

        self.stats_put_interval = float(self.args.stats_interval) / 10

        self.stats = Stats('collector', self.args.case_list)

    def run(self, stats_queue):
        self.stats.reset_all()
        last_stats_printed = time.time()

        time.sleep(self.stats_put_interval)

        collected = []
        ending = False
        while not ending:
            try:
                while not stats_queue.empty():
                    s = stats_queue.get(True, timeout=1) # we should check quickly to get the 'END' message
                    stats_queue.task_done()
                    if s == 'END':
                        #logging.info('stats collect process ending')
                        ending = True
                    else:
                        collected.append(s)
                else:
                    time.sleep(self.stats_put_interval)
            except:
                pass

            now = time.time()
            if now - last_stats_printed > self.args.stats_interval:
                self.output_stats(collected)
                collected = []
                last_stats_printed = now

        self.output_stats(collected)

        logging.info('stats collect process exited')

    def output_stats(self, collected):
        self.stats.reset()

        # increase the count for each processor
        for s in collected:
            self.stats.collect(s)

        self.stats.print_to_log(self.args.stats_interval)

class Worker(object):

    def __init__(self, args):
        self.args = args

        SpringDBClient.set_zone(args.zone)
        self._db = SpringDBClient(args.cluster, args.table, args.socket_timeout)

        if self.args.key_token:
            self.key_prefix = {
                'v': '%s_' % self.args.key_token,
                'z': '%s_' % self.args.key_token,
                'h': '%s_' % self.args.key_token,
                'l': '%s_' % self.args.key_token,
            }
        else:
            r = 'r' if not args.sequence_access else 's'
            if self.args.value_token:
                value_token = str(self.args.value_token)
            else:
                value_token = self.args.value_len
            self.key_prefix = {
                'v': 'v|%s|%s|%s|' % (r, args.key_len, value_token),
                'z': 'z|%s|%s|%s|%s|' % (r, args.key_len, value_token, args.score_digits),
                'h': 'h|%s|%s|%s|%s|' % (r, args.key_len, args.sub_key_len, value_token),
                'l': 'l|%s|%s|%s|' % (r, args.key_len, value_token),
            }

        key_digits_need = math.ceil(math.log(self.args.key_space, 10))
        for data_type in ['v']:
            if self.args.key_len < len(self.key_prefix[data_type]) + \
                                   key_digits_need:
                logging.warning('key_len is too small to hold prefix and key')
                sys.exit(-1)

        if self.args.value_num >= pow(10, self.args.score_digits - 1) * 9:
            logging.warning('value_num exceeds the scope of score_digits. ' \
                  'i.e. valid range is from 1000 - 9999')
            sys.exit(-1)

        self.key_num_in_loop_per_process = self.args.key_num_in_loop / self.args.concurrency
        self.error_count = 0
        
        self.score_base = pow(10, self.args.score_digits - 1)

        self.stats_put_interval = float(self.args.stats_interval) / 10
        self.stats = None # init in run, need worker_idx

        self.worker_idx = 0

        self.key_source = None
        self.key_source_line_count = 0
        self.value_source = None
        self.value_source_line_count = 0

        self.pending_idx = [] # for mget etc

    def idx_convert_to_global(self, idx):
        return idx * self.args.concurrency + self.worker_idx

    def run(self, case_name, worker_idx=0, stats_queue=None):
        #logging.info('worker %d started' % worker_idx)
        self.worker_idx = worker_idx
        self.stats = Stats(str(worker_idx), self.args.case_list)
        self.stats_queue = stats_queue

        if self.args.is_perf_test and self.args.verbose:
            logging.info('worker %d starting... task for me %d' % (worker_idx, self.key_num_in_loop_per_process))

        case = getattr(self, case_name)

        self.last_stats_send = time.time()

        if self.args.sequence_access \
            or (self.args.fill_data and case_name in WRITE_CMD_LIST):
            seq_mode = True
        else:
            seq_mode = False
        if seq_mode:
            idx = self.args.access_key_start / self.args.concurrency
            if self.args.is_perf_test:
                logging.info('worker %d key start from %d' \
                    % (worker_idx, self.idx_convert_to_global(idx)))
        else:
            idx = 0
        count = 0
        while count < self.key_num_in_loop_per_process and idx <= self.args.key_space:
            if seq_mode:
                actual_idx = self.idx_convert_to_global(idx) % self.args.key_space
                case(actual_idx)
                if self.args.is_perf_test and count % 100000 == 0:
                    logging.debug('sampling for seq mode. worker_idx %s idx %s' % (worker_idx, actual_idx))
                idx += 1
            else:
                idx = random.randint(0, self.args.key_space)
                case(idx)

            count += 1

        if self.args.is_perf_test:
            if seq_mode:
                logging.info('worker %d run count %d, end with %d' \
                    % (worker_idx, count, self.idx_convert_to_global(idx - 1)))
            else:
                logging.info('worker %d run count %d' % (worker_idx, count))

        if self.stats_queue:
            self.stats_queue.put(self.stats.get())
            self.stats.reset()

        #logging.info('worker %d exited' % worker_idx)

    def stats_add(self, case, seconds, succeed=True):
        self.stats.add(case, seconds, succeed)
        if not succeed:
            self.error_count += 1
            if self.args.error_limit > 0 and self.error_count >= self.args.error_limit:
                logging.warning('TOO MUCH ERROR, process exit')
                sys.exit(-1)

        # use the stats data to control throughput
        self.stats.throughput_control(self.args.throughput_limit / self.args.concurrency)

        now = time.time()
        if now - self.last_stats_send > self.stats_put_interval: # send frequently
            self.last_stats_send = now
            if self.stats_queue:
                self.stats_queue.put(self.stats.get())
                self.stats.reset()

        if not self.args.is_perf_test:
            sys.stdout.write('.')
            sys.stdout.flush()

    def do_gen_key(self, idx, length, prefix=''):
        if self.args.sequence_access:
            idx = '{%s}' % idx
            zero_len = length - len(prefix) - len(idx)
            return prefix + ('0' * zero_len) + idx
        else:
            base = hash_val(idx, self.args.random_key_salt)
            result = '%s{%s}' % (prefix, base)
            while len(result) < length:
                result += base
            if len(result) > length:
                result = result[:length]
            return result

    def gen_key_from_source_file(self):
        try:
            if not self.key_source:
                self.key_source = open(self.args.key_source_file)
                self.key_source_line_count = 0

            while True:
                line = self.key_source.readline()
                if not line:
                    if self.args.key_source_reuse:
                        logging.info('hit EOF of key source file. reopen it next time')
                        self.key_source.close()
                        self.key_source = None # will reopen it next time
                        return 'DUMMY_key_when_hit_EOF'
                    else:
                        logging.info('hit EOF of key source file. exit')
                        sys.exit(0)
                if self.key_source_line_count % self.args.concurrency == self.worker_idx:
                    self.key_source_line_count += 1
                    break
                self.key_source_line_count += 1

            #if self.args.verbose:
                #print self.worker_idx, self.key_source_line_count - 1, line.strip()
            return line.strip()
        except Exception, ex:
            logging.info('error for key source file. %s' % ex)
            sys.exit(-1)

    def gen_key(self, data_type, idx):
        if self.args.key_source_file:
            return self.key_prefix[data_type] + self.gen_key_from_source_file()
        else:
            return self.do_gen_key(idx, self.args.key_len, self.key_prefix[data_type])

    def gen_sub_key(self, idx):
        return self.do_gen_key(idx, self.args.sub_key_len)

    def do_gen_value(self):
        num = self.args.value_len
        return ''.join(random.choice(string.ascii_uppercase) for _ in range(num))

    def gen_cached_value(self):
        if not self.args.enable_random_value_cache:
            return

        self.cached_value_num = min(self.args.random_value_cache_num, 
                                    self.args.random_value_cache_mem * 1024 * 1024 / self.args.value_len,
                                    self.key_num_in_loop_per_process)
        logging.info('gen cached value, num %d' % self.cached_value_num)

        self.cached_value = []
        for i in range(self.cached_value_num):
            self.cached_value.append(self.do_gen_value())

    def gen_value_from_source_file(self):
        try:
            if not self.value_source:
                self.value_source = open(self.args.value_source_file)
                self.value_source_line_count = 0

            while True:
                line = self.value_source.readline()
                if not line:
                    if self.args.value_source_reuse:
                        logging.info('hit EOF of value source file. reopen it next time')
                        self.value_source.close()
                        self.value_source = None # will reopen it next time
                        return 'DUMMY value when hit EOF'
                    else:
                        logging.info('hit EOF of value source file. exit')
                        sys.exit(0)
                if self.value_source_line_count % self.args.concurrency == self.worker_idx:
                    self.value_source_line_count += 1
                    break
                self.value_source_line_count += 1

            #if self.args.verbose:
                #print self.worker_idx, self.value_source_line_count - 1, line.strip()
            return line.strip()
        except Exception, ex:
            logging.info('error for key source file. %s' % ex)
            sys.exit(-1)

    def gen_value(self):
        if self.args.value_source_file:
            return self.gen_value_from_source_file()
        elif self.args.enable_random_value_cache:
            idx = random.randint(0, self.cached_value_num - 1)
            return self.cached_value[idx]
        else:
            return self.do_gen_value()

    def gen_score(self):
        return random.randint(0, self.args.score_range) + self.score_base

    def gen_zrange_score_range(self):
        # score_range * 8 / 10 : avoid to hit the end of range
        score_min = random.randint(0, self.args.score_range * 8 / 10) + self.score_base
        score_max = score_min + self.args.access_score_range
        return (score_min, score_max)

    #def gen_zrange_offset(self):
        #if self.args.value_num - self.args.access_limit <= 0:
            #return 0
        #else:
            #return random.randint(0, self.args.value_num - self.args.access_limit)

    def set(self, idx):
        key = self.gen_key('v', idx)
        value = self.gen_value()

        try:
            ts = time.time()
            ret = self._db.set(key,value )
            te = time.time()
            self.stats_add('set', te - ts, True)
            if self.args.verbose:
                logging.debug('set, %s %s value_len %d value %s' % (idx, key, len(value), value))
        except Exception, ex:
            te = time.time()
            self.stats_add('set', te - ts, False)

    def get(self, idx):
        key = self.gen_key('v', idx)

        try:
            ts = time.time()
            ret = self._db.get(key)
            if ret is None:
                self.stats.add_hits_misses('get', 0, 1)
                if self.args.verbose:
                    logging.debug('get result is None, idx %d key %s' % (idx, key))
                raise Exception('get result is None')
            else:
                self.stats.add_hits_misses('get', 1, 0)
            te = time.time()
            self.stats_add('get', te - ts, True)
        except Exception, ex:
            te = time.time()
            #logging.exception(ex)
            self.stats_add('get', te - ts, False)

    def mget(self, idx):
        self.pending_idx.append(idx)
        if len(self.pending_idx) < self.args.batch_num:
            return

        keys = [self.gen_key('v', i) for i in self.pending_idx]
        
        try:
            ts = time.time()
            ret = self._db.mget(keys)
            if ret is None:
                logging.debug('mget result is None')
                raise Exception('mget result is None')
            te = time.time()

            miss_count = 0
            for i in range(len(ret)):
                if ret[i] is None:
                    miss_count += 1
                    if self.args.verbose:
                        logging.debug('mget result has None, idx %d key %s' \
                                      % (self.pending_idx[i], keys[i]))
            self.stats.add_hits_misses('mget', len(keys) - miss_count, miss_count)
            self.stats_add('mget', te - ts, True)
        except Exception, ex:
            logging.exception('exception in mget')
            te = time.time()
            self.stats_add('mget', te - ts, False)

        self.pending_idx = []
    
    def getthenset(self, idx):
        key = self.gen_key('v', idx)
        value = self.gen_value()

        try:
            ts = time.time()

            ret = self._db.get(key)
            if ret is None:
                logging.debug('get result is None')

            ret = self._db.set(key,value )

            te = time.time()
            self.stats_add('getthenset', te - ts, True)
        except Exception, ex:
            te = time.time()
            self.stats_add('getthenset', te - ts, False)

    def do_zadd(self, key):
        value = self.gen_value()
        score = self.gen_score()
        try:
            ts = time.time()
            ret = self._db.zadd(key, value, score)
            te = time.time()
            self.stats_add('zadd', te - ts, True)
        except Exception, ex:
            te = time.time()
            self.stats_add('zadd', te - ts, False)

    def zadd(self, idx):
        key = self.gen_key('z', idx)

        if self.args.fill_data:
            while True:
                length = self._db.zcard(key)
                if length >= self.args.value_num:
                    break
                #print idx, length, key
                self.do_zadd(key)
        else:
            self.do_zadd(key)

    def zrangebyscore(self, idx):
        key = self.gen_key('z', idx)

        score_min, score_max = self.gen_zrange_score_range()
        score_max = 3 * self.score_base
        #offset= self.gen_access_offset()
        offset= 0
        limit = self.args.access_limit

        try:
            ts = time.time()
            #print idx, key, score_min, score_max, offset, limit
            ret = self._db.zrangebyscore(key, score_min, score_max,
                                         offset, limit, withscores=True)
            if ret == []:
                logging.debug('zrangebyscore result is empty')
            if self.args.verbose:
                logging.debug('zrangebyscore, %s %s %s %s %s' \
                              % (idx, key, score_min, score_max, len(ret)))
            te = time.time()
            self.stats_add('zrangebyscore', te - ts, True)
        except Exception, ex:
            te = time.time()
            self.stats_add('zrangebyscore', te - ts, False)
            if self.args.verbose:
                logging.exception('zrangebyscore failed')

    def do_hset(self, key, sub_key):
        value = self.gen_value()

        try:
            ts = time.time()
            ret = self._db.hset(key, sub_key, value)
            te = time.time()
            self.stats_add('hset', te - ts, True)
        except Exception, ex:
            te = time.time()
            self.stats_add('hset', te - ts, False)

    def hset(self, idx):
        key = self.gen_key('h', idx)

        if self.args.fill_data:
            sub_idx = 0
            while sub_idx < self.args.value_num:
                sub_key = self.gen_sub_key(sub_idx)
                self.do_hset(key, sub_key)
                if self.args.verbose:
                    logging.debug('hset, fill-data, %s %s %s %s', idx, sub_idx, key, sub_key)
                sub_idx += 1
        else:
            sub_idx = random.randint(0, self.args.value_num)
            sub_key = self.gen_sub_key(sub_idx)
            self.do_hset(key, sub_key)
            if self.args.verbose:
                logging.debug('hset, random, %s %s %s %s', idx, sub_idx, key, sub_key)

    def do_hmset(self, key, sub_key_list):
        value = self.gen_value()

        try:
            ts = time.time()
            ret = self._db.hmset(key, sub_key_list)
            te = time.time()
            self.stats_add('hmset', te - ts, True)
        except Exception, ex:
            te = time.time()
            self.stats_add('hmset', te - ts, False)

    def hmset(self, idx):
        key = self.gen_key('h', idx)

        if self.args.fill_data:
            sub_idx = 0
            while sub_idx < self.args.value_num:
                sub_key_list = {}
                while len(sub_key_list) < self.args.batch_num and sub_idx < self.args.value_num:
                    sub_key = self.gen_sub_key(sub_idx)
                    sub_key_list[sub_key] = self.gen_value()
                    sub_idx += 1

                self.do_hmset(key, sub_key_list)
                if self.args.verbose:
                    logging.debug('hmset, fill-data, %s %s %s', idx, key, len(sub_key_list))
        else:
            sub_key_list = {}
            while len(sub_key_list) < self.args.batch_num:
                sub_idx = random.randint(0, self.args.value_num)
                sub_key = self.gen_sub_key(sub_idx)
                sub_key_list[sub_key] = self.gen_value()
            self.do_hmset(key, sub_key_list)
            if self.args.verbose:
                logging.debug('hmset, random, %s %s %s', idx, key, len(sub_key_list))

    def hget(self, idx):
        key = self.gen_key('h', idx)

        sub_idx = random.randint(0, self.args.value_num)
        sub_key = self.gen_sub_key(sub_idx)

        try:
            ts = time.time()

            ret = self._db.hget(key, sub_key)
            if ret is None:
                logging.debug('hget result is None, idx %d sub_idx %d key %s' % (idx, sub_idx, key))
                raise Exception('hget result is None')

            te = time.time()
            self.stats_add('hget', te - ts, True)
        except Exception, ex:
            te = time.time()
            self.stats_add('hget', te - ts, False)

    def hmget(self, idx):
        key = self.gen_key('h', idx)

        sub_key_list = []
        while len(sub_key_list) < self.args.batch_num:
            sub_idx = random.randint(0, self.args.value_num)
            sub_key_list.append(self.gen_sub_key(sub_idx))

        try:
            ts = time.time()

            ret = self._db.hmget(key, sub_key_list)
            if ret is None or ret is []:
                logging.debug('hmget result is None, idx %d sub_idx %r key %s' % (idx, sub_idx_list, key))
                raise Exception('hmget result is None')

            te = time.time()
            self.stats_add('hmget', te - ts, True)
        except Exception, ex:
            te = time.time()
            self.stats_add('hmget', te - ts, False)

    def do_rpush(self, key):
        value = self.gen_value()
        try:
            ts = time.time()
            ret = self._db.rpush(key, value)
            te = time.time()
            self.stats_add('rpush', te - ts, True)
        except Exception, ex:
            te = time.time()
            self.stats_add('rpush', te - ts, False)

    def rpush(self, idx):
        key = self.gen_key('l', idx)

        if self.args.fill_data:
            while True:
                length = self._db.llen(key)
                if length >= self.args.value_num:
                    break
                #print idx, length, key
                self.do_rpush(key)
        else:
            self.do_rpush(key)

    def lrange(self, idx):
        key = self.gen_key('l', idx)

        #offset= self.gen_access_offset()
        offset= 0
        limit = self.args.access_limit

        try:
            ts = time.time()
            #print idx, key, score_min, score_max, offset, limit
            ret = self._db.lrange(key, offset, offset + limit)
            if ret == []:
                logging.debug('lrange result is empty')
            if self.args.verbose:
                logging.debug('lrange, %s %s %s %s %s' \
                              % (idx, key, offset, limit, len(ret)))
            te = time.time()
            self.stats_add('lrange', te - ts, True)
        except Exception, ex:
            te = time.time()
            self.stats_add('lrange', te - ts, False)
            if self.args.verbose:
                logging.exception('lrange failed')

def parse_args(is_perf_test=True):
    default_table = 'perftest' if is_perf_test else 'sandbox'

    parser = argparse.ArgumentParser(description='springdb performance test tool')
    # use verbose to control logging, will skip log fomating if not verbose
    parser.add_argument('--verbose', action='store_true', default=False,
                        help='print debug info')

    group = parser.add_argument_group('springdb client init')
    group.add_argument('--cluster', type=str, required=True,
                       help='springdb cluster name or server list. ' \
                       'i.e. 127.0.0.1:11211,127.0.0.1:11311')
    group.add_argument('--table', type=str, default=default_table,
                       help='springdb table name')
    group.add_argument('--socket-timeout', type=float, default=3.0,
                       help='socket timeout')
    group.add_argument('--zone', type=str, default='auto',
                       help='springdb conf zone name. i.e. auto, online, offline, test')

    group = parser.add_argument_group('data settings')
    group.add_argument('--key-len', type=int, default=64,
                       help='(all) length for key')
    group.add_argument('--value-len', type=int, default=64,
                       help='(all) length for value')
    group.add_argument('--sub-key-len', type=int, default=64,
                       help='(hash) length for sub key')
    group.add_argument('--score-digits', type=int, default=10, choices=xrange(5, 19),
                       help='(zset) digits number for score, effects the length')
    group.add_argument('--score-range', type=int, default=10240,
                       help='(zset) score range for score')
    group.add_argument('--value-num', type=int, default=64,
                       help='(hash/list/zset) value number')
    group.add_argument('--random-key-salt', type=str, default='',
                       help='(all) salt to generate random key')
    if is_perf_test:
        group.add_argument('--key-space', type=int, default=10240,
                           help='(all) how many different keys')

    if is_perf_test:
        group = parser.add_argument_group('value cache')
        group.add_argument('--enable-random-value-cache', action='store_true', default=False,
                           help='use cached value')
        group.add_argument('--random-value-cache-mem', type=int, default=100,
                           help='max mem size in M of cached value')
        group.add_argument('--random-value-cache-num', type=int, default=102400,
                           help='max number of cached value')

        group = parser.add_argument_group('source of key')
        group.add_argument('--key-source-file', type=str, default=None,
                           help='use file as key source instead of random generator. ' \
                           'one key per line. will ignore key-len and exit if hit the EOF')
        group.add_argument('--key-token', type=str, default=None,
                           help='this token will be part of the key, please specify this ' \
                           'to distinguish different source of data. ' \
                           'if specified, will ignore value-token.')
        group.add_argument('--key-source-reuse', action='store_true', default=False,
                           help='reopen the source file when hit EOF')

        group = parser.add_argument_group('source of value')
        group.add_argument('--value-source-file', type=str, default=None,
                           help='use file as value source instead of random generator. ' \
                           'one value per line. will ignore value-len and exit if hit the EOF')
        group.add_argument('--value-token', type=str, default=None,
                           help='this token will be part of the key, please specify this ' \
                           'to distinguish different source of data')
        group.add_argument('--value-source-reuse', action='store_true', default=False,
                           help='reopen the source file when hit EOF')

    group = parser.add_argument_group('running settings')
    group.add_argument('--loop', type=int, default=1,
                       help='loop number of all cases')
    group.add_argument('--sleep', type=float, default=0.01,
                       help='sleep time between loop')
    group.add_argument('--key-num-in-loop', type=int, default=32,
                       help='operation number in per loop, its shared by all concurrency. ' \
                       'ignore it if in fill-data mode')
    group.add_argument('--error-limit', type=int, default=10,
                       help='exit if hit error limit, 0 for no limit')
    if is_perf_test:
        group.add_argument('--concurrency', type=int, default=1,
                           help='the number of multiple processes')
        group.add_argument('--throughput-limit', type=float, default=0.0,
                           help='throughput limit, 0 means no limit')
        group.add_argument('--sequence-access', action='store_true', default=False,
                           help='randomize key')
        group.add_argument('--fill-data', action='store_true', default=False,
                           help='fill in all key in the key-space, only useful for update cmd')
        group.add_argument('--access-key-start', type=int, default=0,
                           help='start from this key instead of 0')
        group.add_argument('--stats-interval', type=float, default=1.0,
                           help='stats interval in seconds')

    group = parser.add_argument_group('case settings')
    group.add_argument('-c', '--case', default='set',
                       help='test case name')
    group.add_argument('--access-score-range', type=int, default=10240,
                       help='(zset) score range for zrange')
    group.add_argument('--access-limit', type=int, default=100,
                       help='(zset/list) limit for zrange/lrange')
    group.add_argument('--batch-num', type=int, default=16,
                       help='(kv/hash) batch for mget/hmget/hmset etc')

    #group = parser.add_argument_group('specify the key', 'will ignore all case settings')
    #group.add_argument('-k', '--key', type=str, help='test to get the key', default=None)

    args = parser.parse_args()

    # fill in default args for non perf test
    args.is_perf_test = is_perf_test
    if not is_perf_test:
        args.key_space = args.key_num_in_loop
        args.enable_random_value_cache = False
        args.random_value_cache_mem = 100
        args.random_value_cache_num = 102400

        args.key_source_file = None
        args.key_token = None
        args.key_source_reuse = False

        args.value_source_file = None
        args.value_token = None
        args.value_source_reuse = False

        args.concurrency = 1
        args.throughput_limit = 0
        args.sequence_access = True
        args.fill_data = False
        args.access_key_start = 0
        args.stats_interval = 1.0

    FORMAT = '%(asctime)s %(process)d %(levelname)s %(message)s'
    logging.basicConfig(format=FORMAT)
    #logging.getLogger().setLevel(args.log_level)
    logging.getLogger().setLevel(logging.DEBUG)

    return (parser, args)

def test():

    #test
    parser, args = parse_args()
    worker = Worker(args)
    #print worker.gen_key('v', 1)
    #print worker.gen_sub_key(1)
    #perf.set()

    #val = worker.gen_value()
    #import json
    #val = json.dumps({'a': 'abcdefghi', 'b': 'abcdefghi', 'c': 111112222})
    #val = val + val
    #print val
    #import snappy
    #compressed = snappy.compress(val)
    #print len(val), len(compressed)

if __name__ == '__main__':
    test()

# TODO
# perf add value checksum?
# support 1K/1M ...
